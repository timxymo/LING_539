{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from nltk.tokenize.regexp import WordPunctTokenizer\n",
    "import spacy\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pandasql import sqldf\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Calculus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In this sequence of segments,\\nwe review some ...</td>\n",
       "      <td>Probability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>CS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following\\ncontent is provided under a Cre...</td>\n",
       "      <td>Algorithms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The following\\ncontent is provided under a Cre...</td>\n",
       "      <td>Algorithms</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        label\n",
       "0  The following content is\\nprovided under a Cre...     Calculus\n",
       "1  In this sequence of segments,\\nwe review some ...  Probability\n",
       "2  The following content is\\nprovided under a Cre...           CS\n",
       "3  The following\\ncontent is provided under a Cre...   Algorithms\n",
       "4  The following\\ncontent is provided under a Cre...   Algorithms"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/Users/timxymo/Dropbox/UA/2022 Spring/LING 539/Final Project/raw_text.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>The following\\ncontent is provided under a Cre...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Let's explore what happens to\\ndeterminants wh...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>You now know what a\\ntransformation is, so l...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>INTRODUCTION: The\\nfollowing content is provid...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>OPERATOR: The following content\\nis provided u...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text       label\n",
       "23   The following content is\\nprovided under a Cre...  Statistics\n",
       "46   The following\\ncontent is provided under a Cre...  Statistics\n",
       "58   Let's explore what happens to\\ndeterminants wh...  Statistics\n",
       "73   The following content is\\nprovided under a Cre...  Statistics\n",
       "77     You now know what a\\ntransformation is, so l...  Statistics\n",
       "..                                                 ...         ...\n",
       "828  The following content is\\nprovided under a Cre...  Statistics\n",
       "836  The following content is\\nprovided under a Cre...  Statistics\n",
       "838  The following content is\\nprovided under a Cre...  Statistics\n",
       "839  INTRODUCTION: The\\nfollowing content is provid...  Statistics\n",
       "841  OPERATOR: The following content\\nis provided u...  Statistics\n",
       "\n",
       "[79 rows x 2 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['label']=='Statistics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The following content is\\nprovided under a Creative Commons license. Your support will help\\nMIT OpenCourseWare continue to offer high quality\\neducational resources for free. To make a donation or to\\nview additional materials from hundreds of MIT courses,\\nvisit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: So I'm using\\na few things here, right? I'm using the fact that\\nKL is non-negative. But KL is equal to 0 when I\\ntake twice the same argument. So I know that this function\\nis always non-negative. So that's theta and that's\\nKL P theta star P theta. And I know that at theta\\nstar, it's equal to 0. OK? I could be in the case\\nwhere I have this happening. I have two-- let's call\\nit theta star prime. I have two minimizers. That could be the case, right? I'm not saying\\nthat-- so K of L-- KL is 0 at the minimum. That doesn't mean that I\\nhave a unit minimum, right? But it does, actually. What do I need to\\nuse to make sure that I have only one minimum? So the definiteness\\nis guaranteeing to me that there's a unique P\\ntheta star that minimizes it. But then I need to\\nmake sure that there's a unique-- from this\\nunique P theta star, I need to make sure there's\\na unique theta star that defines this P theta star. Exactly. All right, so I\\ncombine definiteness and identifiability to make\\nsure that there is a unique minimizer, in this\\ncase cannot exist. OK, so basically, let me\\nwrite what I just said. So definiteness, that\\nimplies that P theta star is the unique minimizer of P\\ntheta maps to KL P theta star P theta. So definiteness only\\nguarantees that the probability distribution is\\nuniquely identified. And identifiability\\nimplies that theta star is the unique minimizer of\\ntheta maps to KL P theta star P theta, OK? So I'm basically\\ndoing the composition of two injective functions. The first one is the one that\\nmaps, say, theta to P theta. And the second one is\\nthe one that maps P theta to the set of minimizers, OK? So at least morally, you\\nshould agree that theta star is the minimizer of this thing. Whether it's unique\\nor not, you should agree that it's a good one. So maybe you can think\\na little longer on this. So thinking about this\\nbeing the minimizer, then it says,\\nwell, if I actually had a good estimate\\nfor this function, I would use the strategy\\nthat I described for the total\\nvariation, which is, well, I don't know what\\nthis function looks like. It depends on theta star. But maybe I can\\nfind an estimator of this function that\\nfluctuates around this function, and such that when I minimize\\nthis estimator of the function, I'm actually not too far, OK? And this is exactly what\\ndrives me to do this, because I can actually\\nconstruct an estimator. I can actually construct\\nan estimator such that this estimator\\nis actually-- of the KL is actually\\nclose to the KL, all right? So I define KL hat. So all we did is just replacing\\nexpectation with respect to theta star by averages. That's what we did. So if you're a little puzzled by\\nthis error, that's all it says. Replace this guy by this guy. It has no mathematical meaning. It just means just\\nreplace it by. And now that actually tells\\nme how to get my estimator. It just says, well,\\nmy estimator, KL hat, is equal to some constant\\nwhich I don't know. I mean, it certainly\\ndepends on theta star, but I won't care about it\\nwhen I'm trying to minimize-- minus 1/n sum from i from\\n1 to n log f theta of x. So here I'm reading\\nit with the density. You have it with the\\nPMF on the slides, and so you have the two\\nversions in front of you, OK? Oh sorry, I forgot the xi. Now clearly, this function\\nI know how to compute. If you give me a theta, since\\nI know the form of the density f theta, for each\\ntheta that you give me, I can actually compute\\nthis quantity, right? This I don't know,\\nbut I don't care. Because I'm just shifting\\nthe value of the function I'm trying to minimize. The set of minimizers\\nis not going to change. So now, this is my\\nestimation strategy. Minimize in theta KL hat\\nP theta star P theta, OK? So now let's just make sure\\nthat we all agree that-- so what we want is the\\nargument of the minimum, right? arg min means the\\ntheta that minimizes this guy, rather than finding\\nthe value of the min. OK, so I'm trying to find\\nthe arg min of this thing. Well, this is equivalent\\nto finding the arg min of, say, a constant minus\\n1/n sum from i from 1 to n of log f theta of xi. So that's just-- I don't think it likes me. No. OK, so thus minimizing\\nthis average, right? I just plugged in the\\ndefinition of KL hat. Now, I claim that\\ntaking the arg min of a constant plus a function\\nor the arg min of the function is the same thing. Is anybody not comfortable\\nwith this idea? OK, so this is the same. By the way, this\\nI should probably switch to the next slide,\\nbecause I'm writing the same thing, but better. And it's with PMF\\nrather than as PF. OK, now, arg min of the minimum\\nis the same of arg max-- sorry, arg min of\\nthe negative thing is the same as arg max\\nwithout the negative, right? arg max over theta of 1/n from\\ni equal equal 1 to n log f theta of xi. Taking the arg\\nmin of the average or the arg min of\\nthe sum, again, it's not going to make\\nmuch difference. Just adding constants OR\\nmultiplying by constants does not change the\\narg min or the arg max. Now, I have the\\nsum of logs, which is the log of the product. OK? It's the arg max of the\\nlog of f theta of x1 times f theta of x2, f theta of xn. But the log is a function\\nthat's increasing, so maximizing log of a function or\\nmaximizing the function itself is the same thing. The value is going to\\nchange, but the arg max is not going to change. Everybody agrees with this? So this is equivalent to arg\\nmax over theta of pi from 1 to n of f theta xi. And that's because x maps\\nto log x is increasing. So now I've gone from\\nminimizing the KL to minimizing the\\nestimate of the KL to maximizing this product. Well, this chapter is called\\nmaximum likelihood estimation. The maximum comes from the\\nfact that our original idea was to minimize the\\nnegative of a function. So that's why it's\\nmaximum likelihood. And this function here\\nis called the likelihood. This function is really\\njust telling me-- they call it\\nlikelihood because it's some measure of how\\nlikely it is that theta was the parameter that\\ngenerated the data. OK, so let's go to the-- well, we'll go to the formal\\ndefinition in a second. But actually, let\\nme just give you intuition as to why this is\\nthe distribution of the data. Why this is the\\nlikelihood-- sorry. Why is this making sense\\nas a measure of likelihood? Let's now think for simplicity\\nof the following model. So I have-- I'm on the real line\\nand I look at n, say, theta 1 for theta in the\\nreal-- do you see that? OK. Probably you don't. Not that you care. OK, so-- OK, let's look at\\na simple example. So here's the model. As I said, we're looking at\\nobservations on the real line. And they're distributed\\naccording to some n theta 1. So I don't care\\nabout the variance. I know it's 1. And it's indexed by\\ntheta in the real line. OK, so this is-- the only\\nthing I need to figure out is, what is the mean\\nof those guys, OK? Now, I have this n observations. And if you actually remember\\nfrom your probability class, are you familiar with the\\nconcept of joint density? You have multivariate\\nobservations. The joint density of\\nindependent random variables is just a product of their\\nindividual densities. So really, when I look\\nat the product from i equal 1 to n of f\\ntheta of xi, this is really the joint\\ndensity of the vector-- well, let me not use\\nthe word vector-- of x1 xn, OK? So if I take the product of\\ndensity, is it still a density? And it's actually-- but\\nthis time on the r to the n. And so now what this\\nthing is telling me-- so think of it in r2, right? So this is the joint\\ndensity of two Gaussians. So it's something that looks\\nlike some bell-shaped curve in two dimensions. And it's centered at\\nthe value theta theta. OK, they both have\\nthe mean theta. So let's assume for\\none second-- it's going to be hard for me to\\nmake pictures in n dimensions. Actually, already\\nin two dimensions, I can promise you that\\nit's not very easy. So I'm actually\\njust going to assume that n is equal to 1 for\\nthe sake of illustration. OK, so now I have this data. And now I have one\\nobservation, OK? And I know that the f\\ntheta looks like this. And what I'm doing\\nis I'm actually looking at the value of x\\ntheta as my observation. Let's call it x1. Now, my principal tells me,\\njust find the theta that makes this guy the most likely. What is the likelihood of my x1? Well, it's just the\\nvalue of the function. That this value here. And if I wanted to find the most\\nlikely theta that had generated this x1, what I would need\\nto do is to shift this thing and put it here. And so my estimate, my\\nmaximum likelihood estimator here would be theta\\nis equal to x1, OK? That would be just\\nthe observation. Because if I have\\nonly one observation, what else am I going to do? OK, and so it sort\\nof makes sense. And if you have\\nmore observations, you can think of it this way,\\nas if you had more observations. So now I have, say,\\nK observations, or n observations. And what I do is that I\\nlook at the value for each of these guys. So this value, this value,\\nthis value, this value. I take their product and\\nI make this thing large. OK, why do I take the product? Well, because I'm trying\\nto maximize their value all together, and I need to\\njust turn it into one number that I can maximize. And taking the product\\nis the natural way of doing it, either\\nby motivating it by the KL principle\\nor motivating it by maximizing the joint density,\\nrather than just maximizing anything. OK, so that's why, visually,\\nthis is the maximum likelihood. It just says that if my\\nobservations are here, then this guy, this mean theta,\\nis more likely than this guy. Because now if I\\nlook at the value of the function\\nfor this guy-- if I look at theta being\\nthis thing, then this is a very small value. Very small value, very small\\nvalue, very small value. Everything gets a super\\nsmall value, right? That's just the value\\nthat it gets in the tail here, which is very close to 0. But as soon as I start\\ncovering all my points with my bell-shaped curve,\\nthen all the values go up. All right, so I just want\\nto make a short break into statistics,\\nand just make sure that the maximum likelihood\\nprinciple involves maximizing a function. So I just want to\\nmake sure that we're all on par about how do\\nwe maximize functions. In most instances, it's going to\\nbe a one-dimensional function, because theta is going to be\\na one-dimensional parameter. Like here it's the real line. So it's going to be easy. In some cases, it may be\\na multivariate function and it might be\\nmore complicated. OK, so let's just\\nmake this interlude. So the first thing\\nI want you to notice is that if you open any book\\non what's called optimization, which basically is the science\\nbehind optimizing functions, you will talk mostly-- I mean, I'd say\\n99.9% of the cases will talk about\\nminimizing functions. But it doesn't matter, because\\nyou can just flip the function and you just put a minus\\nsign, and minimizing h is the same as maximizing\\nminus h and the opposite, OK? So for this class,\\nsince we're only going to talk about maximum\\nlikelihood estimation, we will talk about\\nmaximizing functions. But don't be lost if\\nyou decide suddenly to open a book on optimization\\nand find only something about minimizing functions. OK, so maximizing an arbitrary\\nfunction can actually be fairly difficult. If I give you a\\nfunction that has this weird shape, right-- let's think of\\nthis polynomial for example-- and I wanted to find the\\nmaximum, how would we do it? So what is the thing you've\\nlearned in calculus on how to maximize the function? Set the derivative equal to 0. Maybe you want to check\\nthe second derivative to make sure it's a\\nmaximum and not a minimum. But the thing is, this is only\\nguaranteeing to you that you have a local one, right? So if I do it for this function,\\nfor example, then this guy is going to satisfy\\nthis criterion, this guy is going to\\nsatisfy this criterion, this guy is going to satisfy\\nthis criterion, this guy here, and this guy satisfies\\nthe criterion, but not the second derivative one. So I have a lot of candidates. And if my function can\\nbe really anything, it's going to be\\ndifficult, whether it's analytically by taking\\nderivatives and setting them to 0, or trying to find\\nsome algorithms to do this. Because if my function\\nis very jittery, then my algorithm basically\\nhas to check all candidates. And if there's a lot of them,\\nit might take forever, OK? So this is-- I have only\\none, two, three, four, five candidates to check. But in practice, you might have\\na million of them to check. And that might take forever. OK, so what's nice about\\nstatistical models, and one of the things that makes all\\nthese models particularly robust, and that we\\nstill talk about them 100 years after they've\\nbeen introduced is that the functions\\nthat-- the likelihoods that they lead\\nfor us to maximize are actually very simple. And they all share\\na nice property, which is that of being concave. All right, so what is\\na concave function? Well, by definition, it's\\njust a function for which-- let's think of it as being\\ntwice differentiable. You can define\\nfunctions that are not differentiable as being concave,\\nbut let's think about it as having a second derivative. And so if you look\\nat the function that has a second derivative,\\nconcave are the functions that have their second\\nderivative that's negative everywhere. Not just at the\\nmaximum, everywhere, OK? And so if it's strictly\\nconcave, this second derivative is actually strictly\\nless than zero. And particularly if I\\nthink of a linear function, y is equal to x,\\nthen this function has its second derivative\\nwhich is equal to zero, OK? So it is concave. But it's not\\nstrictly concave, OK? If I look at the function\\nwhich is negative x squared, what is its second derivative? Minus 2. So it's strictly\\nnegative everywhere, OK? So actually, this is a\\npretty canonical example strictly concave function. If you want to think of a\\npicture of a strictly concave function, think of\\nnegative x squared. So parabola pointing downwards. OK, so we can talk about\\nstrictly convex functions. So convex is just happening when\\nthe negative of the function is concave. So that translates into having\\na second derivative which is either non-negative\\nor positive, depending on whether you're talking about\\nconvexity or strict convexity. But again, those\\nconvex functions are convenient when you're\\ntrying to minimize something. And since we're trying\\nto maximize the function, we're looking for concave. So here are some examples. Let's just go\\nthrough them quickly. OK, so the first one is-- so here I made my\\nlife a little uneasy by talking about the\\nfunctions in theta, right? I'm talking about\\nlikelihoods, right? So I'm thinking of functions\\nwhere the parameter is theta. So I have h of theta. And so if I start\\nwith theta squared, negative theta squared,\\nthen as we said, h prime prime of theta, the\\nsecond derivative is minus 2, which is strictly negative,\\nso this function is strictly concave. OK, another function is\\nh of theta, which is-- what did we pick-- square root of theta. What is the first derivative? 1/2 square root of theta. What is the second derivative? So that's theta to\\nthe negative 1/2. So I'm just picking up\\nanother negative 1/2, so I get negative 1/4. And then I get theta to\\nthe 3/4 downstairs, OK? Sorry, 3/2. And that's strictly negative\\nfor theta, say, larger than 0. And I really need to have\\nthis thing larger than 0 so that it's well-defined. But strictly larger than 0 is\\nso that this thing does not blow up to infinity. And it's true. If you think about this\\nfunction, it looks like this. And already, the first\\nderivative to infinity at 0. And it's a concave function, OK? Another one is the\\nlog, of course. What is the\\nderivative of the log? That's 1 over theta, where h\\nprime of theta is 1 over theta. And the second derivative\\nnegative 1 over theta squared, which again, is negative if\\ntheta is strictly positive. Here I define it as-- I don't need to define it to\\nbe strictly positive here, but I need it for the log. And sine. OK, so let's just do one more. So h of theta is sine of theta. But here I take it\\nonly on an interval, because you want to\\nthink of this function as pointing always downwards. And in particular, you\\ndon't want this function to have an inflection point. You don't want it to\\ngo down and then up and then down and then up,\\nbecause this is not concave. And so sine is certainly\\ngoing up and down, right? So what we do is we restrict\\nit to an interval where sine is actually-- so what does\\nthe sine function looks like at 0, 0? And it's going up. Where is the first\\nmaximum of the sine? STUDENT: [INAUDIBLE] PROFESSOR: I'm sorry. STUDENT: Pi over 2. PROFESSOR: Pi over 2,\\nwhere it takes value 1. And then it goes down again. And then that's at pi. And then I go down again. And here you see I actually\\nstart changing my inflection. So what we do is\\nwe stop it at pi. And we look at this\\nfunction, it certainly looks like a parabola\\npointing downwards. And so if you look at the--\\nyou can check that it actually works with the derivatives. So the derivative\\nof sine is cosine. And the derivative of\\ncosine is negative sine. OK, and this thing between 0\\nand pi is actually positive. So this entire thing is\\ngoing to be negative. OK? And you know, I can come\\nup with a lot of examples, but let's just stick to those. There's a linear\\nfunction, of course. And the find function\\nis going to be concave, but it's actually going to\\nbe convex as well, which means that it's\\ncertainly not going to be strictly concave or convex, OK? So here's your standard picture. And here, if you look\\nat the dotted line, what it tells me is that\\na concave function, and the property we're\\ngoing to be using is that if a strictly concave\\nfunction has a maximum, which is not always the case,\\nbut if it has a maximum, then it actually must be--\\nsorry, a local maximum, it must be a global maximum. OK, so just the fact that\\nit goes up and down and not again means that there's only\\nglobal maximum that can exist. Now if you looked, for example,\\nat the square root function, look at the entire\\npositive real line, then this thing is never\\ngoing to attain a maximum. It's just going to infinity\\nas x goes to infinity. So if I wanted to\\nfind the maximum, I would have to stop\\nsomewhere and say that the maximum is attained\\nat the right-hand side. OK, so that's the beauty about\\nconvex functions or concave functions, is that\\nessentially, these functions are easy to maximize. And if I tell you a\\nfunction is concave, you take the first\\nderivative, set it equal to 0. If you find a point\\nthat satisfies this, then it must be a\\nglobal maximum, OK? STUDENT: What if\\nyour set theta was [INAUDIBLE] then couldn't\\nyou have a function that, by the definition, is concave,\\nwith two upside down parabolas at two disjoint intervals, but\\nyet it has two global maximums? PROFESSOR: So you\\nwon't get them-- so you want the function\\nto be concave on what? On the convex cell\\nof the intervals? Or you want it to be-- STUDENT: [INAUDIBLE] just\\nsaid that any subset. PROFESSOR: OK, OK. You're right. So maybe the\\ndefinition-- so you're pointing to a weakness\\nin the definition. Let's just say that\\ntheta is a convex set and then you're good, OK? So you're right. Since I actually just said that\\nthis is true only for theta, I can just take pieces of\\nconcave functions, right? I can do this, and\\nthen the next one I can do this, on the\\nnext one I can do this. And then I would\\nhave a bunch of them. But what I want is think\\nof it as a global function on some convex set. You're right. So think of theta\\nas being convex for this guy, an interval,\\nif it's a real line. OK, so as I said, for\\nmore generally-- so we can actually define concave\\nfunctions more generally in higher dimensions. And that will be useful\\nif theta is not just one parameter but\\nseveral parameters. And for that, you need to\\nremind yourself of Calculus II, and you have generalization of\\nthe notion of derivative, which is called a gradient, which\\nis basically a vector where each coordinate is just the\\npartial derivative with respect to each coordinate of theta. And the Hessian is\\nthe matrix, which is essentially a generalization\\nof the second derivative. I denote it by nabla\\nsquared, but you can write it the way you want. And so this matrix\\nhere is taking as entry the second partial\\nderivatives of h with respect to theta i and theta j. And so that's the ij-th entry. Who has never seen that? OK. So now, being concave here\\nis essentially generalizing, saying that a vector\\nis equal to zero. Well, that's just setting\\nthe vector-- sorry. The first order condition\\nto say that it's a maximum is going to be the same. Saying that a function has\\na gradient equal to zero is the same as saying that\\neach of its coordinates are equal to zero. And that's actually\\ngoing to be a condition for a global maximum here. So to check convexity, we need\\nto see that a matrix itself is negative. Sorry, to check\\nconcavity, we need to check that a\\nmatrix is negative. And there is a\\nnotion among matrices that compare matrix to zero,\\nand that's exactly this notion. You pre- and post-multiply\\nby the same x. So that works for\\nsymmetric matrices, which is the case here. And so you pre-multiply by x,\\npost-multiply by the same x. So you have your matrix,\\nyour Hessian here. It's a d by d matrix if you\\nhave a d-dimensional matrix. So let's call it-- OK. And then here I\\npre-multiply by x transpose. I post-multiply by x. And this has to be non-positive\\nif I want it to be concave, and strictly negative if I\\nwant it to be strictly concave. OK, that's just a\\nreal generalization. You can check for yourself\\nthat this is the same thing. If I were in dimension 1,\\nthis would be the same thing. Why? Because in dimension 1, pre-\\nand post-multiplying by x is the same as\\nmultiplying by x squared. Because in dimension 1, I can\\njust move my x's around, right? And so that would just\\nmean the first condition would mean in dimension 1 that\\nthe second derivative times x squared has to be less\\nthan or equal to zero. So here I need this for\\nall x's that are not zero, because I can take x to be zero\\nand make this equal to zero, right? So this is for x's that\\nare not equal to zero, OK? And so some examples. Just look at this function. So now I have functions that\\ndepend on two parameters, theta1 and theta2. So the first one is-- so if I take theta\\nto be equal to-- now I need two\\nparameters, r squared. And I look at the function,\\nwhich is h of theta. Can somebody tell me\\nwhat h of theta is? STUDENT: [INAUDIBLE] PROFESSOR: Minus\\n2 theta2 squared? OK, so let's compute the\\ngradient of h of theta. So it's going to be something\\nthat has two coordinates. To get the first\\ncoordinate, what do I do? Well, I take the\\nderivative with respect to theta1, thinking of\\ntheta2 as being a constant. So this thing is\\ngoing to go away. And so I get negative 2 theta1. And when I take the\\nderivative with respect to the second part, thinking\\nof this part as being constant, I get minus 4 theta2. That clear for everyone? That's just the definition\\nof partial derivatives. And then if I want\\nto do the Hessian, so now I'm going to\\nget a 2 by 2 matrix. The first guy here, I take\\nthe first-- so this guy I get by taking the derivative\\nof this guy with respect to theta1. So that's easy. So that's just minus 2. This guy I get by\\ntaking derivative of this guy with\\nrespect to theta2. So I get what? Zero. I treat this guy as\\nbeing a constant. This guy is also\\ngoing to be zero, because I take the derivative\\nof this guy with respect to theta1. And then I take the derivative\\nof this guy with respect to theta2, so I get minus 4. OK, so now I want to check\\nthat this matrix satisfies x transpose-- this matrix x is negative. So what I do is-- so what is x transpose x? So if I do x transpose delta\\nsquared h theta x, what I get is minus 2 x1 squared\\nminus 4 x2 squared. Because this matrix is diagonal,\\nso all it does is just weights the square of the x's. So this guy is\\ndefinitely negative. This guy is negative. And actually, if one\\nof the two is non-zero, which means that x is\\nnon-zero, then this thing is actually strictly negative. So this function is\\nactually strictly concave. And it looks like a\\nparabola that's slightly distorted in one direction. So well, I know this might\\nhave been some time ago. Maybe for some of you might\\nhave been since high school. So just remind yourself of doing\\nsecond derivatives and Hessians and things like this. Here's another one\\nas an exercise. h is minus theta1\\nminus theta2 squared. So this one is going to\\nactually not be diagonal. The Hessian is not\\ngoing to be diagonal. Who would like to do\\nthis now in class? OK, thank you. This is not a calculus class. So you can just do it\\nas a calculus exercise. And you can do it\\nfor log as well. Now, there is a nice\\nrecipe for concavity that works for the second\\none and the third one. And the thing is, if you look\\nat those particular functions, what I'm doing is taking, first\\nof all, a linear combination of my arguments. And then I take a concave\\nfunction of this guy. And this is always\\ngoing to work. This is always going to\\ngive me a complete function. So the computations\\nthat I just made, I actually never made\\nthem when I prepared those slides because I don't have to. I know that if I take a linear\\ncombination of those things and then I take a concave\\nfunction of this guy, I'm always going to\\nget a concave function. OK, so that's an easy way to\\ncheck this, or at least as a sanity check. All right, and so as I said,\\nfinding maximizers of concave or strictly concave\\nfunction is the same as it was in the\\none-dimensional case. What I do-- sorry, in\\nthe one-dimensional case, we just agreed that we\\njust take the derivative and set it to zero. In the high dimensional\\ncase, we take the gradient and set it equal to zero. Again, that's\\ncalculus, all right? So it turns out that\\nso this is going to give me equations, right? The first one is an\\nequation in theta. The second one is an equation\\nin theta1, theta2, theta3, all the way to theta d. And it doesn't mean that because\\nI can write this equation that I can actually solve it. This equation might\\nbe super nasty. It might be like some polynomial\\nand exponentials and logs equal zero, or some crazy thing. And so there's actually,\\nfor a concave function, since we know there's\\na unique maximizer, there's this theory of convex\\noptimization, which really, since those books are\\ntalking about minimizing, you had to find some\\nsort of direction. But you can think of it as the\\ntheory of concave maximization. And they allow you to\\nfind algorithms to solve this numerically and\\nfairly efficiently. OK, that means fast. Even if d is of\\nsize 10,000, you're going to wait for\\none second and it's going to tell you\\nwhat the maximum is. And that's what machine\\nlearning is about. If you've taken any class\\non machine learning, there's a lot of optimization,\\nbecause they have really, really big problems to solve. Often in this\\nclass, since this is more introductory statistics,\\nwe will have a close form. For the maximum\\nlikelihood estimator will be saying theta hat\\nequals, and say x bar, and that will be the maximum\\nlikelihood estimator. So just why-- so has anybody\\nseen convex optimization before? So let me just give\\nyou an intuition why those functions are easy\\nto maximize or to minimize. In one dimension, it's actually\\nvery easy for you to see that. And the reason is this. If I want to maximize the\\nconcave function, what I need to do is to be\\nable to query a point and get as an answer the\\nderivative of this function, OK? So now I said this is the\\nfunction I want to optimize, and I've been running my\\nalgorithm for 5/10 of a second. And it's at this point here. OK, that's the candidate. Now, what I can ask is,\\nwhat is the derivative of my function here? Well, it's going\\nto give me a value. And this value is going to\\nbe either negative, positive, or zero. Well, if it's\\nzero, that's great. That means I'm here\\nand I can just go home. I've solved my problem. I know there's a unique\\nmaximum, and that's what I wanted to find. If it's positive,\\nit actually tells me that I'm on the left\\nof the optimizer. And on the left of\\nthe optimal value. And if it's negative,\\nit means that I'm at the right of the\\nvalue I'm looking for. And so most of the convex\\noptimization methods basically tell you, well,\\nif you query the derivative and it's actually positive,\\nmove to the right. And if it's negative,\\nmove to the left. Now, by how much you\\nmove is basically, well, why people write books. And in higher dimension, it's\\na little more complicated, because in higher dimension,\\nthinks about two dimensions, then I'm only being\\nable to get in a vector. And the vector is only\\ntelling me, well, here is half of the space\\nin which you can move. Now here, if you tell\\nme move to the right, I know exactly which direction\\nI'm going to have to move. But in two dimension,\\nyou're going to basically tell me, well,\\nmove in this global direction. And so, of course, I know\\nthere's a line on the floor I cannot move behind. But even if you tell me,\\ndraw a line on the floor and move only to that\\nside of the line, then there's many directions\\nin that line that I can go to. And that's also why\\nthere's lots of things you can do in optimization. OK, but still, putting this\\nline on the floor is telling me, do not go backwards. And that's very important. It's just telling\\nyou which direction I should be going always, OK? All right, so that's\\nwhat's behind this notion of gradient descent\\nalgorithm, steepest descent. Or steepest descent, actually,\\nif we're trying to maximize. OK, so let's move on. So this course is not about\\noptimization, all right? So as I said, the\\nlikelihood was this guy. The product of f of the xi's. And one way you\\ncan do this is just basically the joint distribution\\nof my data at the point theta. So now the likelihood,\\nformerly-- so here I am giving myself\\nthe model e theta. And here I'm going to\\nassume that e is discrete so that I can talk about PMFs. But everything\\nyou're doing, just redo for the sake of yourself\\nby replacing PMFs by PDFs, and everything's\\ngoing to be fine. We'll do it in a second. All right, so the\\nlikelihood of the model. So here I'm not looking at\\nthe likelihood of a parameter. I'm looking at the\\nlikelihood of a model. So it's actually a\\nfunction of the parameter. And actually, I'm\\ngoing to make it even a function of\\nthe points x1 to xn. All right, so I have a function. And what it takes as\\ninput is all the points x1 to xn and a candidate\\nparameter theta. Not the true one. A candidate. And what I'm going\\nto do is I'm going to look at the probability\\nthat my random variables under this\\ndistribution, p theta, take these exact\\nvalues, px1, px2, pxn. Now remember, if my\\ndata was independent, then I could actually just\\nsay that the probability of this intersection is just a\\nproduct of the probabilities. And it would look\\nsomething like this. But I can define likelihood\\neven if I don't have independent random variables. But think of them as\\nbeing independent, because that's all we're going\\nto encounter in this class, OK? I just want you to be aware that\\nif I had dependent variables, I could still define\\nthe likelihood. I would have to understand how\\nto compute these probabilities there to be able to compute it. OK, so think of\\nBernoullis, for example. So here is my example\\nof a Bernoulli. So my parameter is-- so my model is 0,1 Bernoulli p. p is in the interval 0,1. The probability, just\\nas a side remark, I'm just going to use the\\nfact that I can actually write the PMF of a Bernoulli\\nin a very concise form, right? If I ask you what the\\nPMF of a Bernoulli is, you could tell me, well,\\nthe probability that x-- so under p, the probability that\\nx is equal to 0 is 1 minus p. The probability under p that\\nx is equal to 1 is equal to p. But I can be a bit smart and\\nsay that for any X that's either 0 or 1, the\\nprobability under p that X is equal to\\nlittle x, I can write it in a compact form as p to the\\nX, 1 minus p to the 1 minus x. And you can check that this is\\nthe right form because, well, you have to check it only\\nfor two values of X, 0 and 1. And if you plug in 1,\\nyou only keep the p. If you plug in 0, you\\nonly keep the 1 minus p. And that's just a trick, OK? I could have gone\\nwith many other ways. Agreed? I could have said,\\nactually, something like-- another one would be-- which\\nwe are not going to use, but we could say, well, it's\\nxp plus and minus x 1 minus p, right? That's another one. But this one is going\\nto be convenient. So forget about this\\nguy for a second. So now, I said that\\nthe likelihood is just this function that's computing\\nthe probability that X1 is equal to little x1. So likelihood is L of X1, Xn. So let me try to make\\nthose calligraphic so you know that I'm talking about\\nsmaller values, right? Small x's. x1, xn, and then of course p. Sometimes we even put-- I didn't do it, but\\nsometimes you can actually put a semicolon here, semicolon\\nso you know that those two things are treated differently. And so now you have this\\nthing is equal to what? Well, it's just the\\nprobability under p that X1 is little x1 all\\nthe way to Xn is little xn. OK, that's just the definition. All right, so now\\nlet's start working. So we write the\\ndefinition, and then we want to make it look like\\nsomething we would potentially be able to maximize if I were-- like if I take the derivative\\nof this with respect to p, it's not very helpful\\nbecause I just don't know. Just want the algebraic\\nfunction of p. So this thing is going\\nto be equal to what? Well, what is the first\\nthing I want to use? I have a probability of\\nan intersection of events, so it's just the product\\nof the probabilities. So this is the product from\\ni equal 1 to n of P, small p, Xi is equal to little xi. That's independence. OK, now, I'm starting to mean\\nbusiness, because for each P, we have a closed form, right? I wrote this as this\\nsupposedly convenient form. I still have to reveal to\\nyou why it's convenient. So this thing is equal to-- well, we said that that\\nwas p xi for a little xi. 1 minus p to the 1 minus xi, OK? So that was just what I wrote\\nover there as the probability that Xi is equal to little xi. And since they all have\\nthe same parameter p, just have this p that shows up here. And so now I'm just taking\\nthe products of something to the xi, so it's this\\nthing to the sum of the xi's. Everybody agrees with this? So this is equal to p\\nsum of the xi, 1 minus p to the n minus sum of the xi. If you don't feel comfortable\\nwith writing it directly, you can observe\\nthat this thing here is actually equal to p over\\n1 minus p to the xi times 1 minus p, OK? So now when I take\\nthe product, I'm getting the products\\nof those guys. So it's just this guy\\nto the power of sum and this guy to the power n. And then I can rewrite\\nit like this if I want to And so now-- well,\\nthat's what we have here. And now I am in business\\nbecause I can still hope to maximize this function. And how to maximize\\nthis function? All I have to do is to\\ntake the derivative. Do you want to do it? Let's just take\\nthe derivative, OK? Sorry, I didn't tell you that,\\nwell, the maximum likelihood principle is to just maxim-- the\\nidea is to maximize this thing, OK? But I'm not going to\\nget there right now. OK, so let's do it maybe for\\nthe Poisson model for a second. So if you want to do it\\nfor the Poisson model, let's write the likelihood. So right now I'm\\nnot doing anything. I'm not maximizing. I'm just computing the\\nlikelihood function. OK, so the likelihood\\nfunction for Poisson. So now I know-- what is my\\nsample space for Poisson? STUDENT: Positives. PROFESSOR: Positive integers. And well, let me\\nwrite it like this. Poisson lambda, and I'm going\\nto take lambda to be positive. And so that means that the\\nprobability under lambda that X is equal to little\\nx in the sample space is lambda to the\\nX over factorial x e to the minus lambda. So that's basically the\\nsame as the compact form that I wrote over there. It's just now a different one. And so when I want to\\nwrite my likelihood, again, we said little x's. This is equal to what? Well, it's equal to the\\nprobability under lambda that X1 is little\\nx1, Xn is little xn, which is equal to the product. OK? Just by independence. And now I can write those\\nguys as being-- each of them being i equal 1 to n. So this guy is just this\\nthing where a plug in Xi. So I get lambda to the Xi\\ndivided by factorial xi times e to the minus lambda, OK? And now, I mean, this\\nguy is going to be nice. This guy is not\\ngoing to be too nice. But let's write it. When I'm going to take the\\nproduct of those guys here, I'm going to pick up lambda\\nto the sum of the xi's. Here I'm going to\\npick up exponential minus n times lambda. And here I'm going to\\npick up just the product of the factorials. So x1 factorial all the\\nway to xn factorial. Then I get lambda,\\nthe sum of the xi. Those are little xi's. e to the minus xn lambda. OK? So that might be freaky at\\nthis point, but remember, this is a function we\\nwill be maximizing. And the denominator here\\ndoes not depend on lambda. So we knew that maximizing this\\nfunction with this denominator, or any other\\ndenominator, including 1, will give me the same arg max. So it won't be a problem for me. As long as it does\\nnot depend on lambda, this thing is going to go away. OK, so in the continuous case,\\nthe likelihood I cannot-- right? So if I would write\\nthe likelihood like this in the\\ncontinuous case, this one would be equal to what? Zero, right? So it's not very helpful. And so what we do is we\\ndefine the likelihood as the product of\\nthe f of theta xi. Now that would be a\\njump if I told you, well, just define it\\nlike that and go home and don't discuss it. But we know that this is\\nexactly what's coming from the-- well, actually, I\\nthink I erased it. It was just behind. So this was exactly what\\nwas coming from the KL divergence estimated, right? The thing that I\\nshowed you, if we want to follow this\\nstrategy, which consists in estimating the KL\\ndivergence and minimizing it, is exactly doing this. So in the Gaussian case-- well, let's write it. So in the Gaussian\\ncase, let's see what the likelihood looks like. OK, so if I have a\\nGaussian experiment here-- did I actually write it? OK, so I'm going to take mu and\\nsigma as being two parameters. So that means that my sample\\nspace is going to be what? Well, my sample\\nspace is still R. Those are just my observations. But then I'm going to\\nhave a N mu sigma squared. And the parameters\\nof interest are mu and R. And sigma squared\\nand say 0 infinity. OK, so that's my Gaussian model. Yes. STUDENT: [INAUDIBLE] PROFESSOR: No, there's no-- I mean, there's no difference. STUDENT: [INAUDIBLE] PROFESSOR: Yeah. I think the all the slides\\nI put the curly bracket, then I'm just being lazy. I just like those\\nconcave parenthesis. All right, so let's write it. So the definition, L xi, xn. And now I have two parameters,\\nmu and sigma squared. We said, by definition,\\nis the product from i equal 1 to n of f\\ntheta of little xi. Now, think about it. Here we always had\\nan extra line, right? The line was to say that the\\ndefinition was the probability that they were all\\nequal to each other. That was the joint probability. And here it could actually have\\na line that says it's the joint probability distribution\\nof the xi's. And if it's not\\nindependent, it's not going to be the product. But again, since\\nwe're only dealing with independent observations\\nin the scope of this class, this is the only definition\\nwe're going to be using. OK, and actually,\\nfrom here on, I will literally skip this step\\nwhen I talk about discrete ones as well, because they\\nare also independent. Agreed? So we start with\\nthis, which we agreed was the definition for\\nthis particular case. And so now all of you know by\\nheart what the density of a-- sorry, that's not theta. I should write it\\nmu sigma squared. And so you need to\\nunderstand what this density. And it's product of 1 over\\nsigma square root 2 pi times exponential minus\\nxi minus mu squared divided by 2 sigma squared. OK, that's the Gaussian density\\nwith parameters mu and sigma squared. I just plugged in this thing\\nwhich I don't give you, so you just have to trust me. It's all over any book. Certainly, I mean,\\nyou can find it. I will give it to you. And again, you're not\\nexpected to know it by heart. Though, if you do your homework\\nevery week without wanting to, you will definitely\\nuse some of your brain to remember that thing. OK, and so now, well, I\\nhave this constant in front. 1 over sigma square root\\n2 pi that I can pull out. So I get 1 over sigma square\\nroot 2 pi to the power n. And then I have the product\\nof exponentials, which we know is the exponential of the sum. So this is equal to\\nexponential minus. And here I'm going to put\\nthe 1 over 2 sigma squared outside the sum. And so that's how\\nthis guy shows up. Just the product of the density\\nis evaluated at, respectively, x1 to xn. OK, any questions about\\ncomputing those likelihoods? Yes. STUDENT: Why [INAUDIBLE] PROFESSOR: Oh, that's a typo. Thank you. Because I just took it from\\nprobably the previous thing. So those are\\nactually-- should be-- OK, thank you for\\nnoting that one. So this line should say for\\nany x1 to xn in R to the n. Thank you, good catch. All right, so that's\\nreally e to the n, right? My sample space always. OK, so what is maximum\\nlikelihood estimation? Well again, if you go\\nback to the estimate that we got, the estimation\\nstrategy, which consisted in replacing expectation\\nwith respect to theta star by average of the data\\nin the KL divergence, we would try to maximize\\nnot this guy, but this guy. The thing that we actually\\nplugged in were not any small xi's. Were actually-- the random\\nvariable is capital Xi. So the maximum\\nlikelihood estimator is actually taking\\nthe likelihood, which is a function\\nof little x's, and now the values at which it\\nestimates, if you look at it, is actually-- the capital X is my data. So it looks at the\\nfunction, at the data, and at the parameter theta. That's what the-- so\\nthat's the first thing. And then the maximum\\nlikelihood estimator is maximizing this, OK? So in a way, what it does is\\nit's a function that couples together the data,\\ncapital X1 to capital Xn, with the parameter theta and\\njust now tries to maximize it. So if this is just a\\nlittle hard for you to get, the likelihood is\\nformally defined as a function of x, right? Like when I write f of x. f of little x, I\\ndefine it like that. But really, the only\\nx arguments we're going to evaluate\\nthis function at are always the random\\nvariable, which is the data. So if you want,\\nyou can think of it as those guys being not\\nparameters of this function, but really, random variables\\nthemselves directly. Is there any question? STUDENT: [INAUDIBLE] those\\nrandom variables [INAUDIBLE]?? PROFESSOR: So those are going\\nto be known once you have-- so it's always the\\nsame thing in stats. You first design your\\nestimator as a function of random variables. And then once you get\\ndata, you just plug it in. But we want to think of them\\nas being random variables because we want to understand\\nwhat the fluctuations are. So we're going to keep them as\\nrandom variables for as long as we can. We're going to spit out\\nthe estimator as a function of the random variables. And then when we want\\nto compute it from data, we're just going to plug it in. So keep the random variables\\nfor as long as you can. Unless I give you\\nnumbers, actual numbers, just those are random variables. OK, so there might\\nbe some confusion if you've seen any stats\\nclass, sometimes there's a notation which says,\\noh, the realization of the random variables\\nare lower case versions of the original\\nrandom variables. So lowercase x should be\\nthought as the realization of the upper case X. This\\nis not the case here. When I write this,\\nit's the same way as I write f of x is\\nequal to x squared, right? It's just an argument of a\\nfunction that I want to define. So those are just generic x. So if you correct\\nthe typo that I have, this should say that this\\nshould be for any x and xn. I'm just describing a function. And now the only\\nplace at which I'm interested in evaluating\\nthat function, at least for those first n\\narguments, is at the capital N observations random\\nvariables that I have. So there's actually\\ntexts, there's actually people doing research on when\\ndoes the maximum likelihood estimator exist? And that happens when you\\nhave infinite sets, thetas. And this thing can diverge. There is no global maximum. There's crazy things\\nthat might happen. And so we're actually\\nalways going to be in a case where this maximum\\nlikelihood estimator exists. And if it doesn't, then\\nit means that you actually need to restrict your\\nparameter space, capital Theta, to something smaller. Otherwise it won't exist. OK, so another thing is the\\nlog likelihood estimator. So it is still the\\nlikelihood estimator. We solved before that\\nmaximizing a function or maximizing log\\nof this function is the same thing, because the\\nlog function is increasing. So the same thing is\\nmaximizing a function or maximizing, I don't know,\\nexponential of this function. Every time I take an\\nincreasing function, it's actually the same thing. Maximizing a function\\nor maximizing 10 times this function is the same thing. So the function x maps to\\n10 times x is increasing. And so why do we talk about\\nlog likelihood rather than likelihood? So the log of likelihood\\nis really just-- I mean the log likelihood is\\nthe log of the likelihood. And the reason is exactly\\nfor this kind of reasons. Remember, that was\\nmy likelihood, right? And I want to maximize it. And it turns out that\\nin stats, there's a lot of distributions that look\\nlike exponential of something. So I might as well just\\nremove the exponential by taking the log. So once I have this\\nguy, I can take the log. This is something to\\na power of something. If I take the log, it's\\ngoing to look better for me. I have this thing-- well, I have another\\none somewhere, I think, where I had the Poisson. Where was the Poisson? The Poisson's gone. So the Poisson was\\nthe same thing. If I took the log,\\nbecause it had a power, that would make my life easier. So the log doesn't have any\\nparticular intrinsic notion, except that it's\\njust more convenient. Now, that being\\nsaid, if you think about maximizing the KL,\\nthe original formulation, we actually remove the log. If we come back\\nto the KL thing-- where is my KL? Sorry. That was maximizing the sum\\nof the logs of the pi's. And so then we worked at it by\\nsaying that the sum of the logs was-- maximizing the sum of\\nthe logs was the same as maximizing the product. But here, we're\\nbasically-- log likelihood is just going backwards in\\nthis chain of equivalences. And that's just because\\nthe original formulation was already convenient. So we went to find\\nthe likelihood and then coming back to our\\noriginal estimation strategy. So look at the Poisson. I want to take log here to\\nmake my sum of xi's go down. OK, so this is my estimator. So the log of L-- so one thing that\\nyou want to notice is that the log of L of\\nx1, xn theta, as we said, is equal to the\\nsum from i equal 1 to n of the log of either\\np theta of xi, or-- so that's in the discrete case. And in the continuous\\ncase is the sum of the log of f theta of xi. The beauty of this is that you\\ndon't have to really understand the difference between\\nprobability mass function and probability\\ndistribution function to implement this. Whatever you get,\\nthat's what you plug in. Any questions so far? All right, so shall we\\ndo some computations and check that, actually, we've\\nintroduced all this stuff-- complicate functions,\\nmaximizing, KL divergence, lot of things-- so that we\\ncan spit out, again, averages? All right? That's great. We're going to able\\nto sleep at night and know that there's a really\\npowerful mechanism called maximum likelihood\\nestimator that was actually driving our intuition\\nwithout us knowing. OK, so let's do this so. Bernoulli trials. I still have it over there. OK, so actually, I\\ndon't know what-- well, let me write it like that. So it's P over 1 minus P xi-- sorry, sum of the xi's\\ntimes 1 minus P is to the n. So now I want to maximize\\nthis as a function of P. Well, the first thing\\nwe would want to do is to check that this\\nfunction is concave. And I'm just going to ask\\nyou to trust me on this. So I don't want--\\nsorry, sum of the xi's. I only want to take the\\nderivative and just go home. So let's just take the\\nderivative of this with respect to P. Actually, no. This one was more convenient. I'm sorry. This one was slightly\\nmore convenient, OK? So now we have-- so now let me take the log. So if I take the log, what I get\\nis sum of the xi's times log p plus n minus some of the\\nxi's times log 1 minus p. Now I take the\\nderivative with respect to p and set it equal to zero. So what does that give me? It tells me that sum of the\\nxi's divided by p minus n sum of the xi's divided by\\n1 minus p is equal to 0. So now I need to solve for p. So let's just do it. So what we get is that 1 minus p\\nsum of the xi's is equal to p n minus sum of the xi's. So that's p times n minus sum of\\nthe xi's plus sum of the xi's. So let me put it on the right. So that's p times n is\\nequal to sum of the xi's. And that's equivalent to p-- actually, I should start\\nby putting p hat from here on, because I'm already\\nsolving an equation, right? And so p hat is equal\\nto syn of the xi's divided by n,\\nwhich is my xn bar. Poisson model, as I\\nsaid, Poisson is gone. So let me rewrite it quickly. So Poisson, the likelihood\\nin X1, Xn, and lambda was equal to lambda to\\nthe sum of the xi's e to the minus n lambda\\ndivided by X1 factorial, all the way to Xn factorial. So let me take the\\nlog likelihood. That's going to\\nbe equal to what? It's going to tell me. It's going to be-- well, let me get rid\\nof this guy first. Minus log of X1 factorial\\nall the way to Xn factorial. That's a constant with\\nrespect to lambda. So when I'm going to take the\\nderivative, it's going to go. Then I'm going to have plus sum\\nof the xi's times log lambda. And then I'm going to\\nhave minus n lambda. So now then, you\\ntake the derivative and set it equal to zero. So log L-- well, partial with\\nrespect to lambda of log L, say lambda, equals zero. This is equivalent\\nto, so this guy goes. This guy gives me sum of the\\nxi's divided by lambda hat equals n. And so that's\\nequivalent to lambda hat is equal to sum of the xi's\\ndivided by n, which is Xn bar. Take derivative, set it equal\\nto zero, and just solve. It's a very satisfying\\nexercise, especially when you get the average in the end. You don't have to\\nthink about it forever. OK, the Gaussian model I'm going\\nto leave to you as an exercise. Take the log to get rid\\nof the pesky exponential, and then take the derivative\\nand you should be fine. It's a bit more-- it might be one more\\nline than those guys. OK, so-- well actually,\\nyou need to take the gradient in this case. Don't check the second\\nderivative right now. You don't have to\\nreally think about it. What did I want to add? I think there was\\nsomething I wanted to say. Yes. When I have a function that's\\nconcave and I'm on, like, some infinite\\ninterval, then it's true that taking the derivative\\nand setting it equal to zero will give me the maximum. But again, I might have a\\nfunction that looks like this. Now, if I'm on some finite\\ninterval-- let me go elsewhere. So if I'm on some\\nfinite interval and my function looks like\\nthis as a function of theta-- let's say this is\\nmy log likelihood as a function of theta-- then, OK, there's no\\nplace in this interval-- let's say this is\\nbetween 0 and 1-- there's no place in this interval where\\nthe derivative is equal to 0. And if you actually\\ntry to solve this, you won't find a solution which\\nis not in the interval 0, 1. And that's actually how\\nyou know that you probably should not take the\\nderivative equal to zero. So don't panic if you\\nget something that says, well, the solution is\\nat infinity, right? If this function\\nkeeps going, you will find that\\nthe solution-- you won't be able to find a\\nsolution apart from infinity. You are going to see something\\nlike 1 over theta hat is equal to 0, or\\nsomething like this. So you know that when you've\\nfound this kind of solution, you've probably made a\\nmistake at some point. And the reason is because the\\nfunctions that are like this, you don't find the maximum by\\nsetting the derivative equal to zero. You actually just find\\nthe maximum by saying, well, it's an increasing\\nfunction on the interval 0, 1, so the maximum must\\nbe attained at 1. So here in this\\ncase, that would mean that my maximum would be 1. My estimator would be\\n1, which would be weird. So typically here, you have\\na function of the xi's. So one example that you will see\\nmany times is when this guy is the maximum of the xi's. And in which case, the\\nmaximum is attained here, which is the maximum of this. OK, so just keep in mind-- what I would recommend\\nis every time you're trying to take the\\nmaximum of a function, just try to plot the\\nfunction in your head. It's not too complicated. Those things are usually\\nsquares, or square roots, or logs. You know what those\\nfunctions look like. Just plug them in your\\nmind and make sure that you will find a\\nmaximum which really goes up and then down again. If you don't, then\\nthat means your maximum is achieved at the\\nboundary and you have to think differently to get it. So the machinery that consists\\nin setting the derivative equal to zero works 80% of the time. But o you have to be careful. And from the context,\\nit will be clear that you had to be careful,\\nbecause you will find some crazy stuff, such\\nas solve 1 over theta hat is equal to zero. All right, so\\nbefore we conclude, I just wanted to give you\\nsome intuition about how does the maximum likelihood perform? So there's something called\\nthe Fisher information that essentially controls\\nhow this thing performs. And the Fisher information\\nis, essentially, a second derivative\\nor a Hessian. So if I'm in a one-dimensional\\nparameter case, it's a number, it's a second derivative. If I'm in a multidimensional\\ncase, it's actually a Hessian, it's a matrix. So I'm going to actually take\\nin notation little curly L of theta to be the\\nlog likelihood, OK? And that's the log likelihood\\nfor one observation. So let's call it x generically,\\nbut think of it as being x1, for example. And I don't care\\nof, like, summing, because I'm actually going to\\ntake expectation of this thing. So it's not going to be\\na data driven quantity I'm going to play with. So now I'm going\\nto assume that it is twice differentiable,\\nalmost surely, because it's a random function. And so now I'm going to\\njust sweep under the rug some technical conditions\\nwhen these things hold. So typically, when can I\\npermute integral and derivatives and this kind of stuff that\\nyou don't want to think about? OK, the rule of\\nthumb is it always works until it\\ndoesn't, in which case, that probably means\\nyou're actually solving some sort of calculus problem. Because in practice,\\nit just doesn't happen. So the Fisher information\\nis the expectation of the-- that's called the outer product. So that's the product\\nof this gradient and the gradient transpose. So that forms a matrix, right? That's a matrix minus the outer\\nproduct of the expectations. So that's really what's\\ncalled the covariance matrix of this vector, nabla\\nof L theta, which is a random vector. So I'm forming the covariance\\nmatrix of this thing. And the technical conditions\\ntells me that, actually, this guy, which depends\\nonly on the Hessian, is actually equal to negative\\nexpectation of the-- sorry. It depends on the gradient. Is actually negative\\nexpectation of the Hessian. So I can actually\\nget a quantity that depends on the second\\nderivatives only using first derivatives. But the expectation is\\ngoing to play a role here. And the fact that it's a log. And lots of things\\nactually show up here. And so in this case,\\nwhat I get is that-- so in the one-dimensional\\ncase, then this is just the covariance matrix of\\na one-dimensional thing, which is just a variance of itself. So the variance\\nof the derivative is actually equal to\\nnegative the expectation of the second derivative. OK, so we'll see that next time. But what I wanted to emphasize\\nwith this is that why do we care about this quantity? That's called the\\nFisher information. Fisher is the founding\\nfather of modern statistics. Why do we give this\\nquantity his name? Well, it's because this quantity\\nis actually very critical. What does the second\\nderivative of a function tell me at the maximum? Well, it's telling me\\nhow curved it is, right? If I have a zero second\\nderivative, I'm basically flat. And if I have a very high second\\nderivative, I'm very curvy. And when I'm very\\ncurvy, what it means is that I'm very robust\\nto the estimation error. Remember our\\nestimation strategy, which consisted in replacing\\nexpectation by averages? If I'm extremely curvy,\\nI can move a little bit. This thing, the maximum,\\nis not going to move much. And this formula here-- so forget about the matrix\\nversion for a second-- is actually telling me exactly-- it's telling me the curvature\\nis basically the variance of the first derivative. And so the more the first\\nderivative fluctuates, the more your maximum is\\nactually-- your org max is going to move\\nall over the place. So this is really\\ncontrolling how flat your likelihood, your log\\nlikelihood, is at its maximum. The flatter it is, the more\\nsensitive to fluctuation the arg max is going to be. The curvier it is, the\\nless sensitive it is. And so what we're\\nhoping-- a good model is going to be one that\\nhas a large or small value for the Fisher information. I want this to be-- small? I want it to be large. Because this is the\\ncurvature, right? This number is\\nnegative, it's concave. So if I take a\\nnegative sign, it's going to be something\\nthat's positive. And the larger this thing,\\nthe more curvy it is. Oh, yeah, because\\nit's the variance. Again, sorry. This is what-- OK. Yeah, maybe I should not\\ngo into those details because I'm actually\\nout of time. But just spoiler alert,\\nthe asymptotic variance of your-- the variance,\\nbasically, as n goes to infinity of the\\nmaximum likelihood estimator is going to be 1 over this guy. So we want it to be large,\\nbecause the asymptotic variance is going to be very small. All right, so we're out of time. We'll see that next week. And I have your\\nhomework with me. And I will actually turn it in. I will give it to\\nyou outside so we can let the other room come in. OK, I'll just leave you the--\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[23,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>count(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AI</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Algorithms</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CS</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Calculus</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Structures</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Diff. Eq.</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Linear Algebra</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Math for Eng.</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NLP</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Probability</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Statistics</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label  count(*)\n",
       "0                AI        48\n",
       "1        Algorithms        81\n",
       "2                CS       104\n",
       "3          Calculus        70\n",
       "4   Data Structures        62\n",
       "5         Diff. Eq.        93\n",
       "6    Linear Algebra       152\n",
       "7     Math for Eng.        28\n",
       "8               NLP        19\n",
       "9       Probability       124\n",
       "10       Statistics        79"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "q = \"\"\"SELECT label, count(*) \n",
    "       FROM data\n",
    "       group by label\n",
    "       \"\"\"\n",
    "\n",
    "pysqldf(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AI', 'Statistics', 'NLP', 'Calculus', 'Diff. Eq.', 'Algorithms', 'Data Structures', 'Linear Algebra', 'Math for Eng.', 'Probability', 'CS']\n"
     ]
    }
   ],
   "source": [
    "tags = list(set(data.label))\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Calculus</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In this sequence of segments,\\nwe review some ...</td>\n",
       "      <td>Probability</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>CS</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following\\ncontent is provided under a Cre...</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The following\\ncontent is provided under a Cre...</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        label  \\\n",
       "0  The following content is\\nprovided under a Cre...     Calculus   \n",
       "1  In this sequence of segments,\\nwe review some ...  Probability   \n",
       "2  The following content is\\nprovided under a Cre...           CS   \n",
       "3  The following\\ncontent is provided under a Cre...   Algorithms   \n",
       "4  The following\\ncontent is provided under a Cre...   Algorithms   \n",
       "\n",
       "         gold_label  \n",
       "0              Math  \n",
       "1        Statistics  \n",
       "2  Computer Science  \n",
       "3  Computer Science  \n",
       "4  Computer Science  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "q = \"\"\"SELECT *,\n",
    "       case \n",
    "       when label in ('Linear Algebra', 'Math for Eng.','Diff. Eq.','Calculus') THEN 'Math'\n",
    "       when label in ('CS','Data Structures', 'AI','NLP', 'Algorithms') THEN 'Computer Science'\n",
    "       when label in ('Statistics','Probability') THEN 'Statistics' \n",
    "       END AS gold_label\n",
    "       FROM data\n",
    "       \"\"\"\n",
    "\n",
    "temp = pysqldf(q)\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In this sequence of segments,\\nwe review some ...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following\\ncontent is provided under a Cre...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The following\\ncontent is provided under a Cre...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text             label\n",
       "0  The following content is\\nprovided under a Cre...              Math\n",
       "1  In this sequence of segments,\\nwe review some ...        Statistics\n",
       "2  The following content is\\nprovided under a Cre...  Computer Science\n",
       "3  The following\\ncontent is provided under a Cre...  Computer Science\n",
       "4  The following\\ncontent is provided under a Cre...  Computer Science"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "q = \"\"\"SELECT text, gold_label as label\n",
    "       FROM temp\n",
    "       \"\"\"\n",
    "df = pysqldf(q)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Computer Science', 'Statistics', 'Math']\n"
     ]
    }
   ],
   "source": [
    "tags = list(set(df.label))\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Computer Science', 1: 'Statistics', 2: 'Math'}\n",
      "{'Computer Science': 0, 'Statistics': 1, 'Math': 2}\n"
     ]
    }
   ],
   "source": [
    "index_to_tags_dict = {i:tag for i,tag in enumerate(tags)}\n",
    "tags_to_index_dict = {tag:i for i,tag in enumerate(tags)}\n",
    "\n",
    "print(index_to_tags_dict)\n",
    "print(tags_to_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = spacy.load('en_core_web_sm')\n",
    "stopwords = en.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23769\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = {}\n",
    "sentence_tokens = []\n",
    "for text in df.text:\n",
    "    tokens = WordPunctTokenizer().tokenize(text)\n",
    "#     doc = nlp(text)\n",
    "#     tokens = [token.text for token in doc] # too slow\n",
    "    token_list = []\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        if token not in '''!()-[]{};:'\"\\,<>./?@#$%^&*_~''':\n",
    "            try:\n",
    "                int(token)\n",
    "                continue\n",
    "            except:\n",
    "                token_list.append(token)\n",
    "                bag_of_words[token] = bag_of_words.get(token,0)+1\n",
    "    sentence_tokens.append(token_list)\n",
    "\n",
    "print(len(bag_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 238921,\n",
       " 'following': 1233,\n",
       " 'content': 516,\n",
       " 'is': 121104,\n",
       " 'provided': 464,\n",
       " 'under': 1166,\n",
       " 'a': 108551,\n",
       " 'creative': 395,\n",
       " 'commons': 384,\n",
       " 'license': 426,\n",
       " 'your': 8023,\n",
       " 'support': 703,\n",
       " 'will': 15857,\n",
       " 'help': 918,\n",
       " 'mit': 1775,\n",
       " 'opencourseware': 776,\n",
       " 'continue': 705,\n",
       " 'to': 150006,\n",
       " 'offer': 380,\n",
       " 'high': 1231,\n",
       " 'quality': 434,\n",
       " 'educational': 367,\n",
       " 'resources': 424,\n",
       " 'for': 29891,\n",
       " 'free': 981,\n",
       " 'make': 6679,\n",
       " 'donation': 357,\n",
       " 'or': 17500,\n",
       " 'view': 832,\n",
       " 'additional': 581,\n",
       " 'materials': 363,\n",
       " 'from': 12715,\n",
       " 'hundreds': 394,\n",
       " 'of': 125267,\n",
       " 'courses': 439,\n",
       " 'visit': 673,\n",
       " 'at': 22166,\n",
       " 'ocw': 385,\n",
       " 'edu': 418,\n",
       " 'professor': 5854,\n",
       " 'so': 94931,\n",
       " 'jerison': 31,\n",
       " 'relaxing': 25,\n",
       " 'in': 61516,\n",
       " 'sunny': 5,\n",
       " 'london': 6,\n",
       " 'ontario': 3,\n",
       " 'today': 1471,\n",
       " 'and': 124804,\n",
       " 'sent': 139,\n",
       " 'me': 9235,\n",
       " 'as': 20199,\n",
       " 'his': 466,\n",
       " 'substitute': 232,\n",
       " 'again': 4806,\n",
       " 'i': 100988,\n",
       " 'm': 19522,\n",
       " 'glad': 24,\n",
       " 'here': 29712,\n",
       " 'see': 10623,\n",
       " 'you': 93886,\n",
       " 'all': 22555,\n",
       " 'our': 7144,\n",
       " 'agenda': 12,\n",
       " 'he': 1849,\n",
       " 'said': 2744,\n",
       " 'that': 128624,\n",
       " 'd': 5353,\n",
       " 'already': 2291,\n",
       " 'talked': 744,\n",
       " 'about': 14704,\n",
       " 'power': 1224,\n",
       " 'series': 710,\n",
       " 'taylor': 107,\n",
       " 's': 93564,\n",
       " 'formula': 2237,\n",
       " 'guess': 2187,\n",
       " 'on': 19159,\n",
       " 'last': 3185,\n",
       " 'week': 451,\n",
       " 'right': 18898,\n",
       " 'friday': 104,\n",
       " 'going': 30222,\n",
       " 'go': 9353,\n",
       " 'little': 6505,\n",
       " 'further': 423,\n",
       " 'with': 20553,\n",
       " 'show': 1796,\n",
       " 'some': 12864,\n",
       " 'examples': 1204,\n",
       " 'applications': 311,\n",
       " 'then': 23257,\n",
       " 'have': 41523,\n",
       " 'this': 93391,\n",
       " 'course': 3110,\n",
       " 'evaluation': 110,\n",
       " 'survey': 33,\n",
       " 'll': 11037,\n",
       " 'hand': 1783,\n",
       " 'out': 11254,\n",
       " 'minutes': 443,\n",
       " 'class': 1797,\n",
       " 'also': 5728,\n",
       " 'handout': 91,\n",
       " 'made': 961,\n",
       " 'says': 2570,\n",
       " 'end': 2895,\n",
       " 'term': 2423,\n",
       " 'if': 34877,\n",
       " 'didn': 1802,\n",
       " 't': 27493,\n",
       " 'pick': 1266,\n",
       " 'up': 10831,\n",
       " 'coming': 852,\n",
       " 'grab': 47,\n",
       " 'it': 88597,\n",
       " 'people': 2812,\n",
       " 'tend': 142,\n",
       " 'not': 18782,\n",
       " 'when': 11118,\n",
       " 'they': 11342,\n",
       " 'walk': 578,\n",
       " 're': 23944,\n",
       " 'there': 22861,\n",
       " 'things': 7236,\n",
       " 'missing': 214,\n",
       " 'has': 7996,\n",
       " 'decided': 237,\n",
       " 'office': 102,\n",
       " 'hours': 164,\n",
       " 'be': 33999,\n",
       " 'them': 8089,\n",
       " 'just': 28264,\n",
       " 'hasn': 93,\n",
       " 'check': 1451,\n",
       " 'website': 143,\n",
       " 'information': 1252,\n",
       " 'we': 84103,\n",
       " 'looking': 2324,\n",
       " 'forward': 548,\n",
       " 'final': 907,\n",
       " 'exam': 240,\n",
       " 'which': 16774,\n",
       " 'uh': 88,\n",
       " '--': 26800,\n",
       " 'aren': 295,\n",
       " 'any': 6382,\n",
       " 'questions': 1390,\n",
       " 'technical': 111,\n",
       " 'stuff': 1410,\n",
       " 'let': 19650,\n",
       " 'talk': 2148,\n",
       " 'bit': 3678,\n",
       " 'thought': 559,\n",
       " 'should': 5262,\n",
       " 'review': 298,\n",
       " 'what': 38706,\n",
       " 'story': 396,\n",
       " 'ok': 10967,\n",
       " 'could': 8021,\n",
       " 'attention': 393,\n",
       " 'please': 245,\n",
       " 'way': 10044,\n",
       " 'writing': 703,\n",
       " 'function': 8070,\n",
       " 'sum': 3368,\n",
       " 'integral': 2427,\n",
       " 'powers': 256,\n",
       " 'x': 19775,\n",
       " 'these': 14992,\n",
       " 'a_0': 6,\n",
       " 'a_1': 17,\n",
       " 'are': 26322,\n",
       " 'numbers': 2749,\n",
       " 'an': 14875,\n",
       " 'example': 5334,\n",
       " 'polynomial': 727,\n",
       " 'forgotten': 40,\n",
       " 'one': 28334,\n",
       " 'type': 1201,\n",
       " 'goes': 3061,\n",
       " 'finite': 731,\n",
       " 'number': 8632,\n",
       " 'terms': 2879,\n",
       " 'ends': 295,\n",
       " 'other': 8813,\n",
       " 'higher': 607,\n",
       " 'a_i': 15,\n",
       " 'perfectly': 296,\n",
       " 'good': 4856,\n",
       " 'very': 7814,\n",
       " 'special': 750,\n",
       " 'kind': 4554,\n",
       " 'part': 3084,\n",
       " 'want': 13263,\n",
       " 'tell': 2503,\n",
       " 'behave': 99,\n",
       " 'almost': 888,\n",
       " 'exactly': 2753,\n",
       " 'like': 14500,\n",
       " 'polynomials': 106,\n",
       " 'thing': 9785,\n",
       " 'careful': 422,\n",
       " 'using': 3190,\n",
       " 'isn': 744,\n",
       " 'concern': 32,\n",
       " 'minute': 376,\n",
       " 'think': 7429,\n",
       " 'generalized': 101,\n",
       " 'caution': 18,\n",
       " 'call': 4005,\n",
       " 'r': 3359,\n",
       " 'where': 7903,\n",
       " 'can': 28756,\n",
       " 'between': 3433,\n",
       " 'infinity': 1931,\n",
       " 'inclusive': 6,\n",
       " 'absolute': 499,\n",
       " 'value': 6527,\n",
       " 'less': 2908,\n",
       " 'than': 8295,\n",
       " 'smaller': 1242,\n",
       " 'size': 2174,\n",
       " 'converges': 268,\n",
       " 'bigger': 1348,\n",
       " 'diverges': 36,\n",
       " 'called': 3266,\n",
       " 'radius': 242,\n",
       " 'convergence': 398,\n",
       " 'various': 447,\n",
       " 'well': 12551,\n",
       " 'how': 12575,\n",
       " 'find': 4580,\n",
       " 'but': 27100,\n",
       " 'give': 3964,\n",
       " 'few': 1085,\n",
       " 'more': 8117,\n",
       " 'properties': 642,\n",
       " 'earlier': 331,\n",
       " 'another': 4093,\n",
       " 'inside': 1251,\n",
       " 'its': 3133,\n",
       " 'derivatives': 638,\n",
       " 'does': 5274,\n",
       " 'differentiate': 320,\n",
       " 'over': 12512,\n",
       " 'those': 6666,\n",
       " 'a_n': 10,\n",
       " 'expressed': 91,\n",
       " 'derivative': 2478,\n",
       " 'saying': 2072,\n",
       " 'f': 4783,\n",
       " '),': 543,\n",
       " 'written': 740,\n",
       " 'plus': 9920,\n",
       " 'bracket': 224,\n",
       " 'n': 17968,\n",
       " 'means': 4135,\n",
       " 'take': 8460,\n",
       " 'times': 10766,\n",
       " 'once': 2170,\n",
       " 'divided': 1441,\n",
       " 'by': 16024,\n",
       " '!,': 19,\n",
       " 'multiply': 1844,\n",
       " 'linear': 3187,\n",
       " 'quadratic': 490,\n",
       " 'second': 4044,\n",
       " 'remember': 2364,\n",
       " 'divide': 1049,\n",
       " 'coefficients': 391,\n",
       " 'record': 86,\n",
       " 'values': 2226,\n",
       " '=': 1572,\n",
       " 'computed': 256,\n",
       " 'my': 6807,\n",
       " 'summary': 129,\n",
       " 'did': 3717,\n",
       " 'repeat': 305,\n",
       " 'wasn': 310,\n",
       " 'due': 109,\n",
       " 'david': 291,\n",
       " 'was': 10343,\n",
       " 'leonard': 1,\n",
       " 'euler': 299,\n",
       " 'exponential': 1214,\n",
       " 'e': 4762,\n",
       " 'compute': 2021,\n",
       " 'computation': 425,\n",
       " 'because': 16403,\n",
       " 'such': 1403,\n",
       " 'important': 1886,\n",
       " 'do': 29258,\n",
       " 'order': 4888,\n",
       " 'know': 11481,\n",
       " 'comes': 1367,\n",
       " 'into': 5000,\n",
       " 'down': 4712,\n",
       " 'evaluate': 519,\n",
       " 'get': 16774,\n",
       " 'now': 21871,\n",
       " 'plug': 760,\n",
       " 'wind': 98,\n",
       " 'factorials': 46,\n",
       " 'denominators': 28,\n",
       " 'discovery': 26,\n",
       " 'leonhard': 2,\n",
       " 'something': 8005,\n",
       " 'yes': 1967,\n",
       " 'ma': 17,\n",
       " 'am': 1684,\n",
       " 'audience': 6349,\n",
       " 'far': 1511,\n",
       " 'write': 5078,\n",
       " 'before': 3285,\n",
       " 'becomes': 1090,\n",
       " 'defined': 1024,\n",
       " 'satisfactory': 20,\n",
       " 'solution': 3022,\n",
       " 'problem': 6399,\n",
       " 'suppose': 1453,\n",
       " 'phrase': 216,\n",
       " 'question': 3618,\n",
       " 'until': 1093,\n",
       " 'pattern': 439,\n",
       " 'anyone': 374,\n",
       " 'who': 1603,\n",
       " 'doubt': 33,\n",
       " 'next': 4033,\n",
       " 'might': 3550,\n",
       " 'would': 10997,\n",
       " 'summation': 193,\n",
       " 'convention': 166,\n",
       " 'don': 8148,\n",
       " 'believe': 396,\n",
       " 'enough': 1314,\n",
       " 'clear': 856,\n",
       " 'answer': 3549,\n",
       " 'thank': 415,\n",
       " 'basic': 644,\n",
       " 'oh': 1879,\n",
       " 'whenever': 372,\n",
       " 'say': 11719,\n",
       " 'always': 2884,\n",
       " 'yeah': 3340,\n",
       " 'functions': 1468,\n",
       " 'excellent': 79,\n",
       " 'reasonable': 368,\n",
       " 'expression': 920,\n",
       " 'giving': 456,\n",
       " 'true': 2594,\n",
       " 'complicated': 848,\n",
       " 'occur': 380,\n",
       " 'calculus': 393,\n",
       " 'sines': 90,\n",
       " 'cosines': 91,\n",
       " 'tangents': 28,\n",
       " 'expansions': 9,\n",
       " 'newton': 219,\n",
       " 'expansion': 209,\n",
       " '/(': 24,\n",
       " '+': 1426,\n",
       " ').': 905,\n",
       " 'somewhere': 662,\n",
       " 'along': 1122,\n",
       " 'line': 3437,\n",
       " 'learned': 449,\n",
       " 'geometric': 330,\n",
       " 'tells': 1153,\n",
       " 'alternating': 33,\n",
       " 'may': 1406,\n",
       " 'wonder': 53,\n",
       " 'minuses': 30,\n",
       " 'came': 491,\n",
       " 'really': 7214,\n",
       " 'probably': 1905,\n",
       " 'remembered': 15,\n",
       " 'minus': 12592,\n",
       " 'sign': 787,\n",
       " 'gets': 1594,\n",
       " 'replaced': 137,\n",
       " 'signs': 192,\n",
       " 'maybe': 4032,\n",
       " 'anyway': 734,\n",
       " 'graph': 2179,\n",
       " 'looks': 2202,\n",
       " 'denominator': 482,\n",
       " 'pole': 23,\n",
       " 'indication': 34,\n",
       " 'try': 2823,\n",
       " 'converge': 275,\n",
       " 'infinite': 620,\n",
       " 'putting': 494,\n",
       " 'big': 2239,\n",
       " 'fact': 3204,\n",
       " 'put': 4348,\n",
       " 'keep': 1942,\n",
       " 'getting': 1505,\n",
       " 'every': 2990,\n",
       " 'new': 2615,\n",
       " 'calculate': 1033,\n",
       " 'haven': 875,\n",
       " 'seen': 1536,\n",
       " 'iterated': 47,\n",
       " 'sir': 16,\n",
       " 'blow': 88,\n",
       " 'though': 833,\n",
       " 'seems': 567,\n",
       " 'fine': 640,\n",
       " 'why': 4776,\n",
       " 'smooth': 96,\n",
       " 'innocuous': 4,\n",
       " 'off': 1561,\n",
       " 'direction': 1245,\n",
       " 'happen': 1511,\n",
       " 'look': 8403,\n",
       " 'partial': 1206,\n",
       " 'sums': 231,\n",
       " 'mind': 561,\n",
       " 'even': 3325,\n",
       " 'doesn': 3103,\n",
       " 'still': 2793,\n",
       " 'anything': 1801,\n",
       " 'fail': 144,\n",
       " 'only': 6308,\n",
       " 'two': 13866,\n",
       " 'real': 1816,\n",
       " 'edge': 1656,\n",
       " 'different': 4885,\n",
       " 'trig': 150,\n",
       " 'sine': 965,\n",
       " 'sin': 307,\n",
       " 'start': 4069,\n",
       " 'computing': 434,\n",
       " 'sounds': 247,\n",
       " 'lot': 3047,\n",
       " 'work': 3186,\n",
       " 'cosine': 938,\n",
       " 'third': 1274,\n",
       " 'prime': 2257,\n",
       " 'front': 511,\n",
       " 'cancels': 144,\n",
       " 'follow': 453,\n",
       " 'canceling': 15,\n",
       " 'sudden': 54,\n",
       " 'back': 4169,\n",
       " 'started': 972,\n",
       " 'same': 8759,\n",
       " 'forever': 266,\n",
       " 'ever': 592,\n",
       " 'happens': 2586,\n",
       " 'equals': 4221,\n",
       " 'zeros': 220,\n",
       " 'ones': 1207,\n",
       " 'through': 3379,\n",
       " 'four': 2082,\n",
       " 'fold': 49,\n",
       " 'periodicity': 22,\n",
       " 'multiplied': 400,\n",
       " 'drop': 191,\n",
       " 'coefficient': 375,\n",
       " 'fourth': 474,\n",
       " 'board': 516,\n",
       " 'fifth': 127,\n",
       " 've': 8193,\n",
       " 'gone': 274,\n",
       " 'done': 2680,\n",
       " '!--': 5,\n",
       " '!.': 13,\n",
       " 'guessed': 90,\n",
       " 'alternate': 48,\n",
       " 'exponentials': 112,\n",
       " 'grow': 182,\n",
       " 'fast': 743,\n",
       " 'remark': 47,\n",
       " 'reason': 1353,\n",
       " 'general': 2309,\n",
       " '^(': 73,\n",
       " '2n': 104,\n",
       " ')!.': 2,\n",
       " 'odd': 382,\n",
       " 'fixed': 562,\n",
       " 'fix': 323,\n",
       " 'large': 1376,\n",
       " ')!,': 2,\n",
       " 'sorry': 1588,\n",
       " 'itself': 964,\n",
       " 'numerator': 296,\n",
       " 'each': 5364,\n",
       " 'gives': 1450,\n",
       " 'factorial': 644,\n",
       " 'million': 370,\n",
       " 'first': 10061,\n",
       " 'pretty': 2334,\n",
       " 'billion': 140,\n",
       " 'numerators': 17,\n",
       " 'stay': 434,\n",
       " 'product': 2004,\n",
       " 'multiplying': 519,\n",
       " 'small': 1846,\n",
       " 'no': 5578,\n",
       " 'matter': 1194,\n",
       " 'works': 1348,\n",
       " 'property': 995,\n",
       " 'talks': 40,\n",
       " 'equal': 9685,\n",
       " 'condition': 1076,\n",
       " 'heard': 226,\n",
       " 'got': 5053,\n",
       " 'own': 589,\n",
       " 'everything': 2245,\n",
       " 'hard': 1422,\n",
       " 'periodic': 98,\n",
       " 'obvious': 710,\n",
       " 'hidden': 327,\n",
       " 'away': 1168,\n",
       " 'pi': 1742,\n",
       " 'half': 1624,\n",
       " 'period': 281,\n",
       " 'hide': 20,\n",
       " 'spend': 390,\n",
       " 'telling': 496,\n",
       " 'old': 579,\n",
       " 'operations': 779,\n",
       " ')?': 40,\n",
       " 'actually': 10635,\n",
       " 'too': 1905,\n",
       " 'simple': 2448,\n",
       " 'encourage': 95,\n",
       " 'treat': 141,\n",
       " 'together': 1414,\n",
       " 'distributes': 1,\n",
       " 'radii': 7,\n",
       " 'case': 6844,\n",
       " 'pain': 52,\n",
       " 'long': 1804,\n",
       " 'notice': 1140,\n",
       " 'reflected': 28,\n",
       " 'use': 6735,\n",
       " 'process': 1380,\n",
       " 'differentiation': 116,\n",
       " 'cos': 306,\n",
       " 'differentiating': 82,\n",
       " '3x': 94,\n",
       " '5x': 11,\n",
       " 'cancellation': 26,\n",
       " 'factor': 1596,\n",
       " 'leaves': 362,\n",
       " 'derive': 166,\n",
       " 'calculation': 417,\n",
       " 'taking': 1364,\n",
       " 'cleaner': 43,\n",
       " 'simpler': 375,\n",
       " 'knew': 321,\n",
       " 'add': 2456,\n",
       " 'constant': 3695,\n",
       " 'integrating': 334,\n",
       " 'integrate': 736,\n",
       " 'integration': 469,\n",
       " 'dt': 649,\n",
       " 'anti': 87,\n",
       " 'ln': 243,\n",
       " 'natural': 723,\n",
       " 'log': 5062,\n",
       " 'valid': 432,\n",
       " 'quite': 1218,\n",
       " 'apply': 1040,\n",
       " 'methods': 554,\n",
       " 'plugging': 95,\n",
       " 'wrote': 558,\n",
       " 'change': 2700,\n",
       " 'variable': 3262,\n",
       " 'legal': 57,\n",
       " 'replace': 514,\n",
       " 'discovered': 120,\n",
       " 'began': 28,\n",
       " 'whose': 289,\n",
       " 'pointed': 55,\n",
       " 'bad': 1084,\n",
       " 'cool': 622,\n",
       " 'correct': 1107,\n",
       " 'victory': 23,\n",
       " 'logarithms': 24,\n",
       " 'much': 3693,\n",
       " 'efficient': 271,\n",
       " 'had': 4054,\n",
       " 'appropriate': 201,\n",
       " 'teacher': 33,\n",
       " 'substitution': 267,\n",
       " '^(-': 91,\n",
       " 'concerning': 10,\n",
       " 'define': 1333,\n",
       " 'positive': 2024,\n",
       " 'wouldn': 469,\n",
       " 'worry': 550,\n",
       " 'behaved': 26,\n",
       " 'faster': 415,\n",
       " 'exponentially': 155,\n",
       " 'linearly': 379,\n",
       " 'talking': 1229,\n",
       " 'net': 217,\n",
       " 'outside': 435,\n",
       " 'life': 508,\n",
       " 'inaudible': 3255,\n",
       " 'rather': 772,\n",
       " 'after': 1974,\n",
       " 'expect': 513,\n",
       " 'range': 738,\n",
       " 'both': 2610,\n",
       " 'sure': 1590,\n",
       " 'time': 11863,\n",
       " 'whoops': 79,\n",
       " 'place': 979,\n",
       " 'better': 2443,\n",
       " 'squared': 5327,\n",
       " '(-': 50,\n",
       " ')^': 106,\n",
       " 'several': 359,\n",
       " 'error': 1827,\n",
       " 'appearance': 10,\n",
       " 'normalized': 112,\n",
       " 'square': 2776,\n",
       " 'root': 2666,\n",
       " 'normalization': 124,\n",
       " 'theory': 653,\n",
       " 'probability': 7808,\n",
       " 'calculated': 211,\n",
       " 'point': 6409,\n",
       " 'standard': 983,\n",
       " 'definition': 1906,\n",
       " 'hurts': 7,\n",
       " 'nobody': 195,\n",
       " 'carefully': 209,\n",
       " 'quicker': 11,\n",
       " 'evaluating': 90,\n",
       " '!),': 2,\n",
       " 'imagine': 772,\n",
       " 'form': 2132,\n",
       " 'experience': 121,\n",
       " 'calculator': 62,\n",
       " 'calculates': 11,\n",
       " 'method': 1553,\n",
       " 'sermon': 3,\n",
       " 'ceg': 4,\n",
       " 'wanted': 1128,\n",
       " 'ad': 132,\n",
       " 'were': 3387,\n",
       " 'thinking': 983,\n",
       " 'context': 373,\n",
       " 'vector': 5991,\n",
       " 'learn': 646,\n",
       " 'vectors': 2530,\n",
       " 'explains': 42,\n",
       " 'been': 1869,\n",
       " 'strange': 234,\n",
       " 'formulas': 416,\n",
       " 'rule': 1365,\n",
       " 'quotient': 67,\n",
       " 'sort': 4133,\n",
       " 'random': 4122,\n",
       " 'cases': 1034,\n",
       " 'chain': 973,\n",
       " 'drive': 93,\n",
       " 'home': 249,\n",
       " 'poem': 2,\n",
       " 'drives': 14,\n",
       " 'points': 2530,\n",
       " 'forcefully': 1,\n",
       " 'sequence': 1212,\n",
       " 'segments': 81,\n",
       " 'mathematical': 334,\n",
       " 'background': 85,\n",
       " 'useful': 1036,\n",
       " 'places': 300,\n",
       " 'most': 2745,\n",
       " 'covered': 178,\n",
       " 'exception': 112,\n",
       " 'segment': 166,\n",
       " 'material': 148,\n",
       " 'opportunity': 41,\n",
       " 'refresh': 11,\n",
       " 'concepts': 156,\n",
       " 'intended': 62,\n",
       " 'refresher': 4,\n",
       " 'coverage': 19,\n",
       " 'complete': 541,\n",
       " 'sense': 2163,\n",
       " 'sets': 653,\n",
       " 'definitions': 162,\n",
       " 'related': 543,\n",
       " 'including': 170,\n",
       " 'de': 124,\n",
       " 'morgan': 14,\n",
       " 'laws': 122,\n",
       " 'subtleties': 13,\n",
       " 'arise': 36,\n",
       " 'indexed': 62,\n",
       " 'multiple': 792,\n",
       " 'indices': 131,\n",
       " 'finally': 690,\n",
       " 'sophisticated': 133,\n",
       " 'discussion': 97,\n",
       " 'countable': 60,\n",
       " 'versus': 333,\n",
       " 'uncountable': 26,\n",
       " 'integers': 464,\n",
       " 'fundamentally': 50,\n",
       " 'fundamental': 266,\n",
       " 'difference': 1405,\n",
       " 'reflects': 28,\n",
       " 'probabilistic': 218,\n",
       " 'models': 741,\n",
       " 'involve': 185,\n",
       " 'discrete': 516,\n",
       " 'experiments': 139,\n",
       " 'outcomes': 441,\n",
       " 'continuous': 622,\n",
       " 'everyone': 526,\n",
       " 'lecture': 1299,\n",
       " 'wednesday': 97,\n",
       " 'object': 827,\n",
       " 'oriented': 70,\n",
       " 'programming': 603,\n",
       " 'programmed': 17,\n",
       " 'fairly': 398,\n",
       " 'tough': 42,\n",
       " 'concept': 277,\n",
       " 'grasp': 21,\n",
       " 'hopefully': 542,\n",
       " 'many': 3652,\n",
       " 'code': 1755,\n",
       " 'available': 217,\n",
       " 'lectures': 288,\n",
       " 'hang': 49,\n",
       " 'quickly': 632,\n",
       " 'objects': 372,\n",
       " 'python': 528,\n",
       " 'basically': 2431,\n",
       " 'data': 3733,\n",
       " 'certain': 1020,\n",
       " 'behind': 272,\n",
       " 'scenes': 26,\n",
       " 'representation': 450,\n",
       " 'represents': 218,\n",
       " 'ways': 1332,\n",
       " 'interact': 65,\n",
       " 'specific': 506,\n",
       " 'integer': 590,\n",
       " 'floats': 60,\n",
       " 'strings': 297,\n",
       " 'lists': 398,\n",
       " 'dictionaries': 67,\n",
       " 'types': 390,\n",
       " 'represented': 280,\n",
       " 'idea': 2535,\n",
       " 'saw': 825,\n",
       " 'pass': 389,\n",
       " 'parameter': 664,\n",
       " 'literally': 189,\n",
       " 'kinds': 670,\n",
       " 'create': 788,\n",
       " 'particular': 3072,\n",
       " 'program': 1068,\n",
       " 'created': 248,\n",
       " 'manipulate': 58,\n",
       " 'list': 2903,\n",
       " 'append': 138,\n",
       " 'item': 541,\n",
       " 'delete': 547,\n",
       " 'remove': 335,\n",
       " 'concatenate': 84,\n",
       " 'destroy': 36,\n",
       " 'explicitly': 253,\n",
       " 'elements': 1433,\n",
       " 'forget': 426,\n",
       " 'reassigning': 4,\n",
       " 'collect': 148,\n",
       " 'dead': 107,\n",
       " 'reclaim': 3,\n",
       " 'memory': 1182,\n",
       " 'exploring': 40,\n",
       " 'separate': 382,\n",
       " 'blue': 510,\n",
       " 'car': 171,\n",
       " 'pink': 127,\n",
       " 'abstractions': 21,\n",
       " 'cars': 53,\n",
       " 'blueprint': 12,\n",
       " 'abstraction': 81,\n",
       " 'capture': 192,\n",
       " 'represent': 579,\n",
       " 'wheels': 32,\n",
       " 'doors': 95,\n",
       " 'length': 1700,\n",
       " 'height': 893,\n",
       " 'interface': 43,\n",
       " 'paint': 30,\n",
       " 'color': 551,\n",
       " 'noise': 473,\n",
       " 'noises': 5,\n",
       " 'whereas': 354,\n",
       " 'makes': 1273,\n",
       " 'bring': 280,\n",
       " 'closer': 311,\n",
       " 'worked': 253,\n",
       " 'l': 1472,\n",
       " 'essentially': 1769,\n",
       " 'index': 552,\n",
       " 'element': 1517,\n",
       " 'pointer': 673,\n",
       " 'internally': 22,\n",
       " 'location': 318,\n",
       " 'computer': 913,\n",
       " 'access': 498,\n",
       " 'takes': 1327,\n",
       " 'located': 12,\n",
       " 'reverse': 312,\n",
       " 'soon': 355,\n",
       " 'internal': 127,\n",
       " 'whoever': 56,\n",
       " 'decide': 503,\n",
       " 'implement': 407,\n",
       " 'weren': 117,\n",
       " 'aware': 48,\n",
       " 'need': 5743,\n",
       " 'beauty': 57,\n",
       " 'having': 1084,\n",
       " 'representations': 154,\n",
       " 'private': 56,\n",
       " 'known': 517,\n",
       " 'implemented': 133,\n",
       " 'someone': 591,\n",
       " 'uses': 315,\n",
       " 'able': 1918,\n",
       " 'programs': 223,\n",
       " 'motivation': 75,\n",
       " 'advantages': 33,\n",
       " 'bundle': 9,\n",
       " 'packages': 24,\n",
       " 'exact': 473,\n",
       " 'ultimately': 178,\n",
       " 'contribute': 45,\n",
       " 'decomposition': 141,\n",
       " 'ideas': 313,\n",
       " 'reusable': 6,\n",
       " 'easier': 790,\n",
       " 'read': 813,\n",
       " 'future': 382,\n",
       " 'details': 269,\n",
       " 'implementing': 98,\n",
       " 'figure': 1577,\n",
       " 'name': 1281,\n",
       " 'attributes': 87,\n",
       " 'programmer': 55,\n",
       " 'implements': 24,\n",
       " 'creating': 182,\n",
       " 'figuring': 114,\n",
       " 'instances': 104,\n",
       " 'instance': 519,\n",
       " 'pf': 20,\n",
       " 'defining': 167,\n",
       " 'classes': 324,\n",
       " 'coordinate': 637,\n",
       " 'xy': 252,\n",
       " 'plane': 1481,\n",
       " 'y': 9239,\n",
       " '2d': 155,\n",
       " 'allow': 327,\n",
       " 'us': 3435,\n",
       " 'hey': 226,\n",
       " 'key': 1385,\n",
       " 'word': 2129,\n",
       " 'string': 929,\n",
       " 'parentheses': 258,\n",
       " 'parents': 117,\n",
       " 'parent': 614,\n",
       " 'being': 2222,\n",
       " 'assign': 180,\n",
       " 'variables': 2420,\n",
       " 'therefore': 1088,\n",
       " 'told': 534,\n",
       " 'procedures': 29,\n",
       " 'belong': 149,\n",
       " 'ints': 37,\n",
       " 'procedure': 205,\n",
       " 'except': 908,\n",
       " 'distance': 1036,\n",
       " 'carry': 223,\n",
       " 'generally': 289,\n",
       " 'init': 60,\n",
       " 'underscore': 28,\n",
       " 'df': 105,\n",
       " 'parameters': 711,\n",
       " 'self': 235,\n",
       " 'however': 400,\n",
       " 'trickier': 39,\n",
       " 'yet': 866,\n",
       " 'placeholder': 26,\n",
       " 'refer': 138,\n",
       " 'dot': 1545,\n",
       " 'notation': 1013,\n",
       " 'attribute': 64,\n",
       " 'belongs': 159,\n",
       " 'named': 221,\n",
       " 'stick': 267,\n",
       " 'beyond': 230,\n",
       " 'normal': 1093,\n",
       " 'choose': 1474,\n",
       " 'initialize': 107,\n",
       " 'assignments': 34,\n",
       " 'whatever': 1613,\n",
       " 'passed': 115,\n",
       " 'assigned': 95,\n",
       " 'inits': 1,\n",
       " 'specifications': 18,\n",
       " 'docstring': 8,\n",
       " 'triple': 158,\n",
       " 'quotes': 71,\n",
       " 'specification': 59,\n",
       " 'expected': 1966,\n",
       " 'cert': 1,\n",
       " 'statement': 887,\n",
       " 'force': 456,\n",
       " 'great': 997,\n",
       " 'exercise': 205,\n",
       " 'defines': 81,\n",
       " 'nice': 1566,\n",
       " 'c': 4533,\n",
       " 'previously': 133,\n",
       " 'three': 4681,\n",
       " 'implicitly': 78,\n",
       " 'default': 113,\n",
       " 'passing': 150,\n",
       " 'respectively': 32,\n",
       " 'origin': 523,\n",
       " 'creates': 55,\n",
       " 'used': 1935,\n",
       " 'print': 591,\n",
       " 'lame': 8,\n",
       " 'procedural': 4,\n",
       " 'couple': 1061,\n",
       " 'differences': 189,\n",
       " 'moment': 760,\n",
       " 'calling': 299,\n",
       " 'operator': 425,\n",
       " 'slide': 279,\n",
       " 'perform': 233,\n",
       " 'operation': 661,\n",
       " 'naming': 16,\n",
       " 'euclidean': 49,\n",
       " 'x1': 1301,\n",
       " 'x2': 866,\n",
       " 'y1': 228,\n",
       " 'y2': 139,\n",
       " 'doing': 3945,\n",
       " '2x': 305,\n",
       " 'accessing': 54,\n",
       " 'subtract': 424,\n",
       " 'returns': 180,\n",
       " 'conscious': 9,\n",
       " 'assume': 1407,\n",
       " 'include': 248,\n",
       " 'perimeter': 10,\n",
       " 'equivalent': 494,\n",
       " 'prints': 34,\n",
       " 'understand': 1278,\n",
       " 'cumbersome': 6,\n",
       " 'left': 3540,\n",
       " 'mentioned': 319,\n",
       " 'convert': 252,\n",
       " 'beginning': 602,\n",
       " 'debugging': 87,\n",
       " 'debug': 45,\n",
       " 'funny': 184,\n",
       " 'message': 507,\n",
       " 'uninformative': 8,\n",
       " 'informative': 30,\n",
       " 'starts': 529,\n",
       " 'double': 813,\n",
       " 'underscores': 2,\n",
       " 'str': 28,\n",
       " 'surrounded': 5,\n",
       " 'angle': 584,\n",
       " 'brackets': 92,\n",
       " 'return': 831,\n",
       " 'concatenated': 31,\n",
       " 'comma': 202,\n",
       " 'wrap': 27,\n",
       " 'head': 702,\n",
       " 'around': 1796,\n",
       " 'main': 709,\n",
       " 'whether': 1514,\n",
       " 'words': 2383,\n",
       " 'operators': 123,\n",
       " 'customize': 3,\n",
       " 'functionality': 38,\n",
       " 'addition': 277,\n",
       " 'subtraction': 92,\n",
       " 'greater': 1343,\n",
       " 'bat': 18,\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€™s is not in our bag of words\n",
      "â€™ll is not in our bag of words\n",
      "'re is not in our bag of words\n",
      "thru is not in our bag of words\n",
      "â€™d is not in our bag of words\n",
      "whereafter is not in our bag of words\n",
      "â€™m is not in our bag of words\n",
      "nâ€™t is not in our bag of words\n",
      "'m is not in our bag of words\n",
      "â€™ve is not in our bag of words\n",
      "â€™re is not in our bag of words\n",
      "â€˜ll is not in our bag of words\n",
      "'d is not in our bag of words\n",
      "thence is not in our bag of words\n",
      "thereupon is not in our bag of words\n",
      "nâ€˜t is not in our bag of words\n",
      "n't is not in our bag of words\n",
      "latterly is not in our bag of words\n",
      "â€˜re is not in our bag of words\n",
      "hereafter is not in our bag of words\n",
      "â€˜m is not in our bag of words\n",
      "hereupon is not in our bag of words\n",
      "â€˜d is not in our bag of words\n",
      "whither is not in our bag of words\n",
      "'ve is not in our bag of words\n",
      "herein is not in our bag of words\n",
      "seeming is not in our bag of words\n",
      "â€˜ve is not in our bag of words\n",
      "â€˜s is not in our bag of words\n",
      "whence is not in our bag of words\n",
      "'s is not in our bag of words\n",
      "'ll is not in our bag of words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23475"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete the stop words\n",
    "for word in stopwords:\n",
    "    try:\n",
    "        del bag_of_words[word]\n",
    "    except:\n",
    "        print(word,\"is not in our bag of words\")\n",
    "\n",
    "len(bag_of_words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22504"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = [token for token in bag_of_words.keys() if len(token) <= 2]\n",
    "\n",
    "for word in bigrams:\n",
    "    try:\n",
    "        del bag_of_words[word]\n",
    "    except:\n",
    "        print(word,\"is not in our bag of words\")\n",
    "\n",
    "len(bag_of_words)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10025"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_subset = {key: value for key, value in bag_of_words.items() if value >= 5}\n",
    "n = len(a_subset)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_words = sorted(bag_of_words.items(), key = lambda item: item[1], reverse = True)[:n]\n",
    "top_n_words_to_index = {item[0]:i for i,item in enumerate(top_n_words)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'professor professor jerison relaxing sunny london today sent substitute glad agenda today said talked power series taylor formula guess week right friday going little examples applications course evaluation survey hand minutes class handout says end term didn pick coming grab going people tend pick walk grab going things missing decided office hours end term hasn decided check website information looking forward final exam aren questions technical stuff right let talk power series little bit thought review story power series attention power series way writing function sum integral powers a_0 a_1 numbers example power series polynomial forgotten type power series goes finite number terms ends higher a_i perfectly good example power series special kind power series want tell today power series behave exactly like polynomials thing careful power series isn concern polynomials minute think generalized polynomials thing careful number caution number infinity number infinity inclusive absolute value smaller size sum converges sum sum converges finite value bigger absolute value sum diverges called radius convergence examples radius convergence powers series find let properties power series think professor jerison talked earlier radius convergence inside radius convergence function derivatives derivatives like polynomial differentiate terms derivatives number a_n power series expressed terms value derivative called taylor formula saying inside radius convergence function looking written value function a_0 plus value derivative bracket means derivative times derivative divided multiply linear term power series quadratic term second derivative remember divide multiply terms coefficients power series record values derivatives function computed way let think end summary things talked think example repeat example power series example wasn david jerison euler example function exponential function let let compute repeat computation power series important thing order know derivative second derivative comes taylor formula coefficients know derivative way way derivatives evaluate value value value way derivatives value plug formula find plus plus plus plus numbers wind factorials denominators power series discovery euler yes audience writing power series far write professor far write power series defined satisfactory solution exam problem suppose way phrase question pattern pattern doubt term people tell write summation convention thing don believe right terms clear good answer audience yes thank professor basic example let basic example power series yes way write power series radius convergence tell radius convergence power series infinity sum converges value little minutes yeah audience functions written power series professor functions written power series excellent question function reasonable expression written power series giving good answer true answer little bit complicated functions occur calculus like sines cosines tangents power series expansions examples let example example guess example example think newton euler let find power series expansion function think line learned geometric series tells tells answer write geometric series tells function written alternating sum powers wonder minuses came think geometric series probably remembered minus sign gets replaced minus signs think maybe jerison talked basic example remember graph function looks like little problem denominator graph pole goes infinity indication radius convergence infinity try converge infinite number putting big problem fact getting term gets bigger bigger converge example radius convergence let new example way calculate numbers taylor formula haven seen check calculate iterated derivatives function plug yes sir audience radius convergence blow like fine professor questions problem problem graph perfectly smooth finite excellent question problem radius direction problem radius convergence happen let look partial sums mind partial sum doesn infinity converge audience professor things fail converge example real numbers edge right let different example trig function sine going compute power series expansion sin going taylor formula taylor formula says start computing derivatives sin sounds like going lot work let derivative sine cosine derivative cosine second derivative sine remember minus sin want derivative sine derivative sine prime prime derivative decided derivative sine cosine cosine minus sign want differentiate cosine minus sine sign cancels minus sign sin follow lot canceling sudden right started pattern repeat forever higher higher derivatives sines plus minus sines cosines taylor formula says substitute happens let equals sine cosine sine minus cosine minus started pattern repeat values derivatives zeros plus minus ones pattern fold periodicity write sin taylor formula formula value derivative multiplied second derivative divided second derivative going drop term derivative remember denominator coefficient fourth derivative board drop term fifth term fifth power derivative gone pattern value iterated derivative tell terms pattern guess term let write !-- minus plus guessed power series expansion sine sign alternate denominators big don exponentials grow fast let remark infinity radius convergence power series infinity let reason general term going like odd number write want size happens size goes infinity let think fixed let fix number look powers think size expression gets large let second write like times sorry times times multiplied times numerator multiplied numbers denominator gives factorial written like fixed maybe million big fixed happens numbers pretty big gets maybe billion right denominators getting bigger bigger numerators stay product far going multiplying small numbers matter numbers converge smaller smaller gets bigger sign inside radius convergence sign series converges value works convergence fixed tells radius convergence infinity formula fact property radius convergence talks equal infinity condition number infinity absolute value convergence general term works radius convergence infinity kind fast think heard earlier got sine function new function power series way computing sin terms good evaluation sin tells lot function sin example formula hard sine periodic obvious hidden away expression number half period clear power series power series good things hide properties functions want spend minutes telling power series new power series new power series old called operations power series things power series things multiply example want compute power series sin power series sin power series actually function simple polynomial polynomial a_1 coefficients power series simple sin powers series want encourage treat power series like polynomials multiply operations compute power series sin multiply let right minus plus radius convergence going smaller radii convergence equals infinity case multiply power series pain power series long pretty simple thing notice way know odd functions sine odd function odd function product odd functions function reflected fact powers occur power series odd function like sine powers occur odd powers true multiply differentiate let case use process differentiation find power series cos writing cos derivative sine differentiating term term expression power series sine differentiate term term power series cosine let derivative derivative denominator derivative denominator cancellation happens minus cancels factor factorial leaves cancels factor factorial leaves denominator power series expansion cosine got powers alternate factorials denominator course derive expression taylor formula kind calculation taking higher higher derivatives cosine periodic pattern derivatives values derivatives cleaner way simpler way knew derivative sine differentiate radius convergence multiply add multiply constant things like integrating half course isn let integrate integration going integral integral function find anti derivative evaluate evaluate natural log valid way bigger don want think like smaller going try apply power series methods find use integral find power series natural log plugging expression power series know wrote board change variable minus plus minus thing inside integral legal integrate term term let going evaluate integrate integrate sorry integrate gives replace equals discovered minus plus minus power series expansion began power series radius convergence began power series radius convergence going function pointed function goes bad problem happens reflected radius convergence cool integrate correct power series expansion victory euler use kind power series expansion calculate natural logarithms efficient way people property think substitute appropriate substitute teacher tell substitution going try find power series expansion ^(- way taking power series expansion substitution expansion question audience concerning radius convergence define positive wouldn radius convergence right professor like worry function perfectly behaved large power series fail converge large suppose bigger bigger bigger powers grow infinity grow large faster numbers grow exponentially grow linearly general term bigger general term infinity function talking log net plus perfectly good power series good outside radius convergence fact life yes audience inaudible professor talk class question smaller radii convergence basic answer expect bigger smaller power series gives information inside range function audience inaudible professor case radii convergence infinity radius convergence infinity sure sin infinity case let going integrate end time today power series expansion power series expansion going function right variable taking expansion putting terms whoops place series expansion work little bit better going minus squared going plus minus sign denominator signs going alternate powers denominators factorials times course gone error function appearance error function guess gets normalized putting square root integral ^(- normalization gets large value error function important theory probability think calculated fact point course standard definition error function square root let calculate power series expansion square root hurts want integrate ^(- going use power series expansion going write think carefully example little quicker integrate term term integrating powers pretty simple evaluating minus plus integrating denominator plus imagine guess exactly form began talking multiply coefficient square root coefficient times square root perfectly good way write power series expansion good way compute value error function new function experience calculator probably calculates calculator probably method examples things power series going thing minute professor jerison wanted case thinking taking term lot things course context thing vector calculus learn vectors things like comes explains things course little bit strange like strange formulas product rule quotient rule sort random formulas things learn special cases chain rule drive point home wanted drives points home think'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Processed Subtitles\n",
    "\n",
    "subtitles = []\n",
    "for token_list in sentence_tokens:\n",
    "    sub = []\n",
    "    for token in token_list:\n",
    "        if top_n_words_to_index.get(token,-1) != -1:\n",
    "            sub.append(token)\n",
    "    sub = \" \".join(sub)\n",
    "    subtitles.append(sub)\n",
    "\n",
    "# remove the sountrack watermark \n",
    "temp_sub = [subtitle.replace(\"following content provided creative commons license support help mit opencourseware continue offer high quality educational resources free donation view additional materials hundreds mit courses visit mit opencourseware ocw mit edu \",'') for subtitle in subtitles]\n",
    "subtitles = temp_sub\n",
    "subtitles[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(860, 9997)\n",
      "(860, 9997)\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Word Binary, Word Count and Tfidf Features for each sentence\n",
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "X = vectorizer.fit_transform(subtitles)\n",
    "word_count_features = np.array(X.toarray())\n",
    "\n",
    "vectorizer2 = TfidfVectorizer()\n",
    "X2 =  vectorizer2.fit_transform(subtitles)\n",
    "tfidf_features = np.array(X2.toarray())\n",
    "\n",
    "print(word_count_features.shape)\n",
    "print(tfidf_features.shape)\n",
    "\n",
    "print(word_count_features[0])\n",
    "print(tfidf_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0, 1, 2, 1, 0, 2, 2, 0, 2, 0, 1, 0, 2, 2, 1, 2, 2, 0, 1, 2, 2, 2, 2, 0, 0, 2, 1, 0, 2, 2, 0, 0, 1, 0, 0, 2, 2, 2, 1, 1, 0, 0, 0, 2, 2, 0, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 1, 0, 0, 0, 0, 2, 2, 2, 2, 0, 2, 2, 1, 2, 0, 0, 0, 0, 2, 2, 2, 2, 1, 2, 0, 0, 1, 1, 2, 0, 1, 2, 0, 2, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 0, 2, 1, 1, 0, 1, 2, 0, 0, 1, 2, 2, 1, 1, 2, 1, 0, 1, 1, 0, 2, 1, 0, 0, 0, 2, 2, 1, 0, 0, 2, 0, 2, 0, 2, 1, 2, 0, 0, 0, 2, 0, 0, 0, 0, 2, 2, 1, 1, 2, 0, 1, 0, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 2, 1, 0, 1, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 1, 1, 2, 0, 0, 2, 2, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 2, 0, 0, 2, 0, 1, 1, 0, 2, 2, 2, 1, 0, 2, 2, 2, 1, 0, 2, 2, 2, 0, 1, 0, 2, 1, 0, 2, 2, 0, 2, 0, 2, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 2, 0, 0, 1, 2, 2, 0, 1, 1, 0, 2, 1, 1, 0, 0, 1, 0, 0, 2, 0, 0, 1, 2, 0, 0, 0, 1, 0, 1, 2, 0, 1, 2, 1, 2, 2, 2, 2, 0, 0, 1, 2, 2, 2, 0, 1, 2, 0, 0, 0, 1, 2, 0, 2, 1, 1, 2, 0, 2, 2, 1, 1, 1, 0, 1, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 1, 2, 2, 1, 0, 1, 1, 1, 0, 2, 0, 2, 0, 2, 2, 0, 1, 0, 0, 0, 2, 0, 1, 2, 2, 1, 0, 0, 2, 0, 2, 0, 2, 2, 2, 1, 0, 1, 1, 0, 0, 1, 1, 2, 0, 1, 2, 2, 2, 0, 2, 0, 0, 0, 2, 2, 0, 0, 2, 2, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 2, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 2, 2, 0, 2, 1, 2, 0, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1, 2, 0, 1, 0, 1, 2, 0, 2, 2, 0, 2, 0, 2, 0, 1, 1, 2, 2, 0, 0, 1, 1, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 2, 1, 0, 1, 2, 2, 1, 2, 2, 2, 2, 0, 1, 2, 0, 2, 2, 2, 2, 0, 0, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 1, 2, 2, 0, 2, 2, 1, 1, 0, 0, 2, 1, 1, 2, 1, 1, 1, 0, 2, 2, 0, 1, 2, 1, 2, 1, 0, 0, 0, 2, 2, 0, 1, 1, 0, 1, 1, 0, 1, 2, 0, 0, 2, 2, 0, 2, 0, 2, 0, 0, 2, 2, 1, 2, 1, 0, 2, 1, 0, 1, 1, 2, 2, 2, 2, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 0, 1, 2, 0, 2, 2, 2, 0, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 2, 2, 2, 0, 1, 0, 1, 0, 0, 2, 1, 1, 0, 2, 0, 0, 0, 0, 2, 0, 2, 1, 2, 2, 2, 2, 0, 2, 0, 0, 1, 1, 1, 1, 0, 2, 0, 0, 0, 2, 2, 0, 1, 2, 1, 0, 0, 1, 1, 2, 0, 2, 2, 2, 2, 2, 1, 2, 0, 0, 0, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 1, 2, 0, 2, 2, 1, 0, 2, 2, 2, 2, 1, 2, 0, 0, 0, 2, 0, 1, 0, 2, 0, 2, 2, 0, 1, 2, 2, 2, 0, 0, 1, 1, 0, 2, 1, 2, 1, 2, 1, 0, 2, 2, 2, 0, 0, 2, 2, 2, 0, 2, 1, 2, 2, 2, 0, 2, 2, 1, 2, 2, 1, 0, 2, 0, 0, 2, 1, 0, 1, 0, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 0, 1, 1, 0, 1, 2, 2, 2, 2, 0, 0, 2, 2, 0, 1, 1, 1, 2, 2, 0, 0, 0, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 0, 1, 2, 0, 0, 1, 0, 0, 0, 2, 1, 0, 1, 2, 1, 0, 0, 2, 1, 0, 2, 2, 1, 0, 2, 0, 1, 0, 1, 1, 1, 1, 2, 0, 2, 0, 2, 0, 0, 2, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "860"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [tags_to_index_dict[label] for label in df.label]\n",
    "print(labels)\n",
    "\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>count(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Computer Science</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Math</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Statistics</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label  count(*)\n",
       "0  Computer Science       314\n",
       "1              Math       343\n",
       "2        Statistics       203"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "q = \"\"\"SELECT label, count(*) \n",
    "       FROM df\n",
    "       group by label\n",
    "       \"\"\"\n",
    "\n",
    "pysqldf(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 860*0.7 = 602\n",
    "train = 600\n",
    "labels[:train].count(0) #224\n",
    "labels[:train].count(1) #145\n",
    "labels[:train].count(2) #231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[train:].count(0) #90\n",
    "labels[train:].count(1) #58\n",
    "labels[train:].count(2) #112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = df.iloc[:600,]\n",
    "testing = df.iloc[600:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>Let's keep building our table of\\nLaplace tr...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>Hello everyone. So far in the series on\\ndata ...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>Say I have some matrix a --\\nlet's say a is n ...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>In this lesson, we're going to write code\\nto ...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>&amp;gt;&amp;gt; [MUSIC] &amp;gt;&amp;gt; DAVID J. MALAN: All ...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>The following content is\\nprovided by MIT Open...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text             label\n",
       "600  The following content is\\nprovided under a Cre...              Math\n",
       "601    Let's keep building our table of\\nLaplace tr...  Computer Science\n",
       "602  Hello everyone. So far in the series on\\ndata ...        Statistics\n",
       "603  Say I have some matrix a --\\nlet's say a is n ...  Computer Science\n",
       "604  In this lesson, we're going to write code\\nto ...        Statistics\n",
       "..                                                 ...               ...\n",
       "855  The following content is\\nprovided under a Cre...              Math\n",
       "856  &gt;&gt; [MUSIC] &gt;&gt; DAVID J. MALAN: All ...              Math\n",
       "857  The following content is\\nprovided by MIT Open...              Math\n",
       "858  The following content is\\nprovided under a Cre...  Computer Science\n",
       "859  The following content is\\nprovided under a Cre...              Math\n",
       "\n",
       "[260 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Binary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multiclass Logistic Classifier for Word Binary Features\n",
    "# clf_logistic_wb = LogisticRegression(solver = 'lbfgs', multi_class = 'multinomial', max_iter=1000)\n",
    "# clf_logistic_wb = clf_logistic_wb.fit(word_binary_features[:train], labels[:train])\n",
    "# pred_logistic_wb = clf_logistic_wb.predict(word_binary_features[train:])\n",
    "# accuracy_logistic_wb = np.mean(pred_logistic_wb==labels[train:])*100\n",
    "# print(\"Accuracy =\", accuracy_logistic_wb)\n",
    "\n",
    "# cm = confusion_matrix(labels[train:], pred_logistic_wb)\n",
    "# # ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mutliclass Naive Bayes Classifier for Word Binary Features\n",
    "# clf_nb_wb = GaussianNB()\n",
    "# clf_nb_wb = clf_nb_wb.fit(word_binary_features[:train], labels[:train])\n",
    "# pred_nb_wb = clf_nb_wb.predict(word_binary_features[train:])\n",
    "# accuracy_nb_wb = np.mean(pred_nb_wb==labels[train:])*100\n",
    "# print(\"Accuracy =\", accuracy_nb_wb)\n",
    "\n",
    "# cm = confusion_matrix(labels[train:], pred_nb_wb)\n",
    "# # ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random Forest Classifier for Word Binary Features\n",
    "# clf_rf_wb = RandomForestClassifier(max_depth=6, random_state=0, n_estimators=100, criterion='gini')\n",
    "# clf_rf_wb = clf_rf_wb.fit(word_binary_features[:train], labels[:train])\n",
    "# pred_rf_wb = clf_rf_wb.predict(word_binary_features[train:])\n",
    "# accuracy_rf_wb = np.mean(pred_rf_wb==labels[train:])*100\n",
    "# print(\"Accuracy =\",accuracy_rf_wb)\n",
    "\n",
    "# cm = confusion_matrix(labels[train:], pred_rf_wb)\n",
    "# cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # AdaBoost Classifier for Word Count Features\n",
    "# clf_ada_wb = AdaBoostClassifier(n_estimators=100,learning_rate=1.0)\n",
    "# clf_ada_wb = clf_ada_wb.fit(word_binary_features[:train], labels[:train])\n",
    "# pred_ada_wb = clf_ada_wb.predict(word_binary_features[train:])\n",
    "# accuracy_ada_wb = np.mean(pred_ada_wb==labels[train:])*100\n",
    "# print(\"Accuracy =\",accuracy_ada_wb)\n",
    "\n",
    "# cm = confusion_matrix(labels[train:], pred_ada_wb)\n",
    "# # ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 41.15384615384615\n"
     ]
    }
   ],
   "source": [
    "# Multiclass Logistic Classifier for Word Count Features\n",
    "clf_logistic_wc = LogisticRegression(solver = 'lbfgs', multi_class = 'multinomial', max_iter=1000)\n",
    "clf_logistic_wc = clf_logistic_wc.fit(word_count_features[:train], labels[:train])\n",
    "pred_logistic_wc = clf_logistic_wc.predict(word_count_features[train:])\n",
    "accuracy_logistic_wc = np.mean(pred_logistic_wc==labels[train:])*100\n",
    "print(\"Accuracy =\", accuracy_logistic_wc)\n",
    "\n",
    "cm = confusion_matrix(labels[train:], pred_logistic_wc)\n",
    "# ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 33.07692307692307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[32, 23, 35],\n",
       "       [32,  4, 22],\n",
       "       [41, 21, 50]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mutliclass Naive Bayes Classifier for Word Count Features\n",
    "clf_nb_wc = GaussianNB()\n",
    "clf_nb_wc = clf_nb_wc.fit(word_count_features[:train], labels[:train])\n",
    "pred_nb_wc = clf_nb_wc.predict(word_count_features[train:])\n",
    "accuracy_nb_wc = np.mean(pred_nb_wc==labels[train:])*100\n",
    "print(\"Accuracy =\",accuracy_nb_wc)\n",
    "\n",
    "cm = confusion_matrix(labels[train:], pred_nb_wc)\n",
    "cm\n",
    "# ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 40.38461538461539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[34,  0, 56],\n",
       "       [20,  0, 38],\n",
       "       [41,  0, 71]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Classifier for Word Count Features\n",
    "clf_rf_wc = RandomForestClassifier(max_depth=8, random_state=0, n_estimators=300, criterion='gini')\n",
    "clf_rf_wc = clf_rf_wc.fit(word_count_features[:train], labels[:train])\n",
    "pred_rf_wc = clf_rf_wc.predict(word_count_features[train:])\n",
    "accuracy_rf_wc = np.mean(pred_rf_wc == labels[train:])*100\n",
    "print(\"Accuracy =\",accuracy_rf_wc)\n",
    "\n",
    "cm = confusion_matrix(labels[train:], pred_rf_wc)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 35.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[37, 14, 39],\n",
       "       [29,  3, 26],\n",
       "       [49, 12, 51]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AdaBoost Classifier for Word Count Features\n",
    "clf_ada_wc = AdaBoostClassifier(n_estimators=100,learning_rate=1.0)\n",
    "clf_ada_wc = clf_ada_wc.fit(word_count_features[:train], labels[:train])\n",
    "pred_ada_wc = clf_ada_wc.predict(word_count_features[train:])\n",
    "accuracy_ada_wc = np.mean(pred_ada_wc==labels[train:])*100\n",
    "print(\"Accuracy =\",accuracy_ada_wc)\n",
    "\n",
    "cm = confusion_matrix(labels[train:], pred_ada_wc)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 35.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.14615385, 0.04230769, 0.15769231],\n",
       "       [0.08846154, 0.        , 0.13461538],\n",
       "       [0.18461538, 0.04230769, 0.20384615]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiclass Logistic Classifier for Tfidf Features\n",
    "clf_logistic_tfidf = LogisticRegression(solver = 'lbfgs', multi_class = 'multinomial', max_iter=500)\n",
    "clf_logistic_tfidf = clf_logistic_tfidf.fit(tfidf_features[:train], labels[:train])\n",
    "pred_logistic_tfidf = clf_logistic_tfidf.predict(tfidf_features[train:])\n",
    "accuracy_logistic_tfidf = np.mean(pred_logistic_tfidf==labels[train:])*100\n",
    "print(\"Accuracy =\",accuracy_logistic_tfidf)\n",
    "\n",
    "cm = confusion_matrix(labels[train:], pred_logistic_tfidf, normalize='all')\n",
    "cm\n",
    "# ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 32.30769230769231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.12692308, 0.07692308, 0.14230769],\n",
       "       [0.13076923, 0.00384615, 0.08846154],\n",
       "       [0.16538462, 0.07307692, 0.19230769]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mutliclass Naive Bayes Classifier for Tfidf Features\n",
    "clf_nb_tfidf = GaussianNB()\n",
    "clf_nb_tfidf = clf_nb_tfidf.fit(tfidf_features[:train], labels[:train])\n",
    "pred_nb_tfidf = clf_nb_tfidf.predict(tfidf_features[train:])\n",
    "accuracy_nb_tfidf = np.mean(pred_nb_tfidf==labels[train:])*100\n",
    "print(\"Accuracy =\",accuracy_nb_tfidf)\n",
    "\n",
    "cm = confusion_matrix(labels[train:], pred_nb_tfidf, normalize='all')\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 43.84615384615385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.12692308, 0.        , 0.21923077],\n",
       "       [0.09230769, 0.        , 0.13076923],\n",
       "       [0.11923077, 0.        , 0.31153846]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Classifier for Tfidf Features\n",
    "clf_rf_tfidf = RandomForestClassifier(max_depth=5, random_state=0, n_estimators=300, criterion='gini')\n",
    "clf_rf_tfidf = clf_rf_tfidf.fit(tfidf_features[:train], labels[:train])\n",
    "pred_rf_tfidf = clf_rf_tfidf.predict(tfidf_features[train:])\n",
    "accuracy_rf_tfidf = np.mean(pred_rf_tfidf==labels[train:])*100\n",
    "print(\"Accuracy =\",accuracy_rf_tfidf)\n",
    "\n",
    "cm = confusion_matrix(labels[train:], pred_rf_tfidf, normalize='all')\n",
    "cm\n",
    "# ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 38.84615384615385\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost Classifier for Tfidf Features\n",
    "clf_ada_tfidf = AdaBoostClassifier(n_estimators=100,learning_rate=1.0)\n",
    "clf_ada_tfidf = clf_ada_tfidf.fit(tfidf_features[:train], labels[:train])\n",
    "pred_ada_tfidf = clf_ada_tfidf.predict(tfidf_features[train:])\n",
    "accuracy_ada_tfidf = np.mean(pred_ada_tfidf==labels[train:])*100\n",
    "print(\"Accuracy =\",accuracy_ada_tfidf)\n",
    "\n",
    "cm = confusion_matrix(labels[train:], pred_ada_tfidf, normalize='all')\n",
    "# ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pysqldf = lambda q: sqldf(q, globals())\n",
    "# q = \"\"\"SELECT text,\n",
    "#        CASE WHEN label = 'Computer Science' THEN 0\n",
    "#             WHEN label = 'Statistics' THEN 1\n",
    "#             WHEN label = 'Math' THEN 2\n",
    "#        END AS num_label\n",
    "#        FROM test\n",
    "#        \"\"\"\n",
    "\n",
    "# test = pysqldf(q)\n",
    "\n",
    "# test['pred_logistic_tfidf'] = pred_logistic_tfidf\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>Let's keep building our table of\\nLaplace tr...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>Hello everyone. So far in the series on\\ndata ...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>Say I have some matrix a --\\nlet's say a is n ...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>In this lesson, we're going to write code\\nto ...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>&amp;gt;&amp;gt; [MUSIC] &amp;gt;&amp;gt; DAVID J. MALAN: All ...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>The following content is\\nprovided by MIT Open...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text             label\n",
       "600  The following content is\\nprovided under a Cre...              Math\n",
       "601    Let's keep building our table of\\nLaplace tr...  Computer Science\n",
       "602  Hello everyone. So far in the series on\\ndata ...        Statistics\n",
       "603  Say I have some matrix a --\\nlet's say a is n ...  Computer Science\n",
       "604  In this lesson, we're going to write code\\nto ...        Statistics\n",
       "..                                                 ...               ...\n",
       "855  The following content is\\nprovided under a Cre...              Math\n",
       "856  &gt;&gt; [MUSIC] &gt;&gt; DAVID J. MALAN: All ...              Math\n",
       "857  The following content is\\nprovided by MIT Open...              Math\n",
       "858  The following content is\\nprovided under a Cre...  Computer Science\n",
       "859  The following content is\\nprovided under a Cre...              Math\n",
       "\n",
       "[260 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 1, 0, 1, 2, 0, 2, 2, 2, 0, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 2, 2, 2, 0, 1, 0, 1, 0, 0, 2, 1, 1, 0, 2, 0, 0, 0, 0, 2, 0, 2, 1, 2, 2, 2, 2, 0, 2, 0, 0, 1, 1, 1, 1, 0, 2, 0, 0, 0, 2, 2, 0, 1, 2, 1, 0, 0, 1, 1, 2, 0, 2, 2, 2, 2, 2, 1, 2, 0, 0, 0, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 1, 2, 0, 2, 2, 1, 0, 2, 2, 2, 2, 1, 2, 0, 0, 0, 2, 0, 1, 0, 2, 0, 2, 2, 0, 1, 2, 2, 2, 0, 0, 1, 1, 0, 2, 1, 2, 1, 2, 1, 0, 2, 2, 2, 0, 0, 2, 2, 2, 0, 2, 1, 2, 2, 2, 0, 2, 2, 1, 2, 2, 1, 0, 2, 0, 0, 2, 1, 0, 1, 0, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 0, 1, 1, 0, 1, 2, 2, 2, 2, 0, 0, 2, 2, 0, 1, 1, 1, 2, 2, 0, 0, 0, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 0, 1, 2, 0, 0, 1, 0, 0, 0, 2, 1, 0, 1, 2, 1, 0, 0, 2, 1, 0, 2, 2, 1, 0, 2, 0, 1, 0, 1, 1, 1, 1, 2, 0, 2, 0, 2, 0, 0, 2, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_label = labels[train:]\n",
    "print(gold_label)\n",
    "# len(gold_label) 260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2,\n",
       "       2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 0, 0, 2, 0, 0, 2, 2, 0, 0, 2,\n",
       "       2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 0, 0, 2, 0, 2, 2, 2, 2, 2, 2, 0, 0,\n",
       "       2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 0, 2, 2, 2, 0, 2, 2, 0,\n",
       "       0, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0,\n",
       "       0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 0, 2, 2, 0, 2,\n",
       "       2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0,\n",
       "       2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 0, 2,\n",
       "       0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 2, 0, 0, 0, 2, 2, 0, 0, 2, 2, 2, 2,\n",
       "       2, 0, 2, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_rf_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2,\n",
       "       0, 2, 2, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 2, 0,\n",
       "       2, 2, 2, 0, 2, 1, 0, 2, 2, 0, 1, 0, 2, 0, 2, 0, 2, 2, 2, 1, 2, 0,\n",
       "       2, 0, 0, 2, 0, 2, 2, 0, 2, 2, 2, 0, 0, 1, 0, 2, 0, 0, 0, 2, 2, 1,\n",
       "       0, 2, 2, 2, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 2, 0, 0, 2,\n",
       "       2, 0, 2, 2, 2, 0, 0, 1, 0, 0, 2, 2, 0, 0, 2, 2, 0, 2, 0, 0, 0, 0,\n",
       "       2, 1, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0,\n",
       "       1, 1, 0, 2, 2, 2, 2, 0, 1, 1, 0, 0, 2, 0, 2, 0, 2, 2, 1, 0, 0, 2,\n",
       "       2, 2, 0, 2, 0, 2, 2, 2, 0, 0, 0, 2, 0, 0, 2, 1, 2, 2, 2, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 2, 0, 0, 2, 2, 2, 0, 0, 2,\n",
       "       0, 2, 2, 0, 0, 2, 2, 1, 0, 1, 2, 2, 0, 0, 2, 2, 0, 0, 2, 0, 2, 0,\n",
       "       1, 0, 2, 0, 2, 2, 0, 2, 0, 0, 0, 0, 2, 2, 2, 2, 0, 1])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ada_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 2 | ada - 1 | rf - 2 | logistic - 0 | gold - 1\n",
      "index 12 | ada - 2 | rf - 0 | logistic - 1 | gold - 1\n",
      "index 13 | ada - 2 | rf - 0 | logistic - 1 | gold - 0\n",
      "index 33 | ada - 2 | rf - 0 | logistic - 1 | gold - 0\n",
      "index 79 | ada - 0 | rf - 2 | logistic - 1 | gold - 0\n",
      "index 96 | ada - 0 | rf - 2 | logistic - 1 | gold - 2\n",
      "index 107 | ada - 0 | rf - 2 | logistic - 1 | gold - 0\n",
      "index 108 | ada - 0 | rf - 2 | logistic - 1 | gold - 2\n",
      "index 111 | ada - 1 | rf - 2 | logistic - 0 | gold - 0\n",
      "index 122 | ada - 1 | rf - 2 | logistic - 0 | gold - 0\n",
      "index 153 | ada - 1 | rf - 2 | logistic - 0 | gold - 1\n",
      "index 154 | ada - 1 | rf - 0 | logistic - 2 | gold - 0\n",
      "index 163 | ada - 2 | rf - 0 | logistic - 1 | gold - 2\n",
      "index 165 | ada - 0 | rf - 2 | logistic - 1 | gold - 2\n",
      "index 169 | ada - 1 | rf - 0 | logistic - 2 | gold - 2\n",
      "index 185 | ada - 0 | rf - 2 | logistic - 1 | gold - 0\n",
      "index 194 | ada - 0 | rf - 2 | logistic - 1 | gold - 2\n",
      "index 208 | ada - 0 | rf - 2 | logistic - 1 | gold - 1\n",
      "index 239 | ada - 0 | rf - 2 | logistic - 1 | gold - 1\n",
      "index 240 | ada - 2 | rf - 0 | logistic - 1 | gold - 1\n",
      "index 257 | ada - 1 | rf - 2 | logistic - 0 | gold - 2\n"
     ]
    }
   ],
   "source": [
    "# word count\n",
    "for i in range(1,len(gold_label)):\n",
    "    if pred_ada_wc[i] != pred_rf_wc[i] and pred_ada_wc[i] != pred_logistic_wc[i] and pred_rf_wc[i] != pred_logistic_wc[i]:\n",
    "        print('index', i, '| ada -', pred_ada_wc[i], '| rf -', pred_rf_wc[i], '| logistic -', pred_logistic_wc[i], '| gold -', gold_label[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 63 | ada - 1 | rf - 2 | logistic - 0 | gold - 1\n",
      "index 95 | ada - 1 | rf - 2 | logistic - 0 | gold - 2\n",
      "index 96 | ada - 0 | rf - 2 | logistic - 1 | gold - 2\n",
      "index 98 | ada - 0 | rf - 2 | logistic - 1 | gold - 0\n",
      "index 107 | ada - 0 | rf - 2 | logistic - 1 | gold - 0\n",
      "index 108 | ada - 0 | rf - 2 | logistic - 1 | gold - 2\n",
      "index 144 | ada - 0 | rf - 2 | logistic - 1 | gold - 2\n",
      "index 154 | ada - 1 | rf - 0 | logistic - 2 | gold - 0\n",
      "index 163 | ada - 1 | rf - 2 | logistic - 0 | gold - 2\n",
      "index 227 | ada - 1 | rf - 2 | logistic - 0 | gold - 2\n"
     ]
    }
   ],
   "source": [
    "# tfidf\n",
    "for i in range(1,len(gold_label)):\n",
    "    if pred_ada_tfidf[i] != pred_rf_tfidf[i] and pred_ada_tfidf[i] != pred_logistic_tfidf[i] and pred_rf_tfidf[i] != pred_logistic_tfidf[i]:\n",
    "        print('index', i, '| ada -', pred_ada_tfidf[i], '| rf -', pred_rf_tfidf[i], '| logistic -', pred_logistic_tfidf[i], '| gold -', gold_label[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'video little formal defining vector vector addition scalar multiplication video want kind basics lot examples tangible sense vectors operate let define couple vectors going vectors going video going easy draw remember set tuples ordered tuples numbers know looks like comma real numbers member reals member reals sense means right coordinate axes wanted plot know view coordinate imagine axis second coordinate plotted vertical axis traditionally axis second number axis visually represent literally single point plane continue infinity direction points number lines immediately kind bigger space said wouldn abstract examples let vectors going let define vector nice bold vector equal numbers negative vector nice bold let don know vectors let add based definition vector addition stay color don switching forth nice deep plus bolded equal add terms negative plus plus definition vector addition going equal fair came definition vector addition represent vector know coordinates know coordinate convention way way visualize things wanted plot point coordinate axes point horizontal traditionally axis direction convention second point vertical direction point sorry let clear right right point right standard convention convention representing vectors tempted maybe represent vector point minus level second convention vectors start point let dealing dimensional vectors start point let starting point point represent vector draw line point point let let wanted draw minus representing want represent vector minus plus confusing draw obvious let want start point let reasons pick random point pick point right starting point minus want represent vector said add term vector coordinate plus minus minus new going minus going let starting point minus comma want represent draw arrow minus plus term minus plus second term plus minus comma minus comma point right draw line vector look like draw line draw arrow end point representation vector minus actually let little bit better minus actually little closer right minus comma right draw vector like remember point minus comma arbitrary place draw vector started point started point comma thing gone minus horizontal direction movement horizontal direction plus vertical direction drawn minus horizontal plus vertical gets right easily drawn vector like interpretations vector draw color vector vector light blue color right vector vector little arrow notation vector vectors draw infinite number vector draw vector draw like vector goes vector right similarly vector vector pick arbitrary point vector goes right goes right goes vector representation vector looks like start right right representation vector infinite number representations convention called standard position start initial point let write standard position start vectors draw vector standard position start like vector standard position right vector standard position let write vector standard position right vectors standard position things drew valid let interpretation happened added plus draw vector standard position calculated right draw standard position looks like vector right look vector right vector plus standard position draw like clear relationship added relationship head tails means tail end end remember valid representations representations vector parallel start equally valid representation vector start point right kind end point vector standard position draw vector starting right vector drawn like interesting happened remember vector representation standard position equally valid way represent vector add right connect starting point end point addition added vectors started end point started gone right drawn right like add plus starting point end point visual representation plus sure confirms number went right went got plus let think happens scale vectors multiply times scalar factor let pick new vectors gotten monotonous let define vector vector let equal wanted draw vector standard position horizontal vertical vector standard position wanted non standard position right right like equally valid way drawing vector equally valid way happens multiply vector don know times times vector going equal times terms going times times times vector look like let start arbitrary position let start right going right times vector looks like times vector look pointing exact direction twice long makes sense scaled factor multiply scalar changing direction direction exact thing scaling draw drawn right drawn right seen don want cover seen goes exactly case draw standard position line twice far twice long exact direction happens multiply minus times vector equal minus times minus minus times minus new vector minus minus minus times vector let start arbitrary point let standard position right left left looks like new vector going look like let try draw relatively straight line minus times vector draw little arrow sure know vector happened kind direction actually exact opposite direction line right exact opposite direction negative right flipped multiplied negative times flipped right right multiplied negative scaled times long negative flips flips backwards notion kind start understanding idea subtracting vectors let new vectors right let vector nice bold equal video examples let vector equal let vector nice bold equal negative minus want think notion minus equal thing plus minus times vector right plus minus times vector use definitions know multiply scalar equal let switch colors don like color equal vector minus times minus times minus times minus minus times minus minus going vectors added right adding minus minus vector minus going equal let looks like visually represent vector standard position looks like vector vector standard position let different color green vector minus minus looks like actually ended inadvertently collinear vectors hey interesting vector difference vector vector let draw someplace start vector looks like difference vectors minus hey difference shift actually start straight difference end points kind connecting end points actually didn want draw collinear vectors let example kind interesting don book let define vector case let define vector minus minus standard position look like vector start origin vector look like orange minus minus vector looks like minus know view plus minus times minus minus think idea way time wanted basic definitions scalar multiplication minus going equal plus minus times minus minus minus thing plus minus minus right difference vector draw add like vector look like shouldn curve like minus drew like example showed draw heads look like start point right right end right difference vectors let sure difference vectors looks like looks like kind sense intuitively minus difference vectors view difference vector vector right like know let kind second grade world scalars minus equal tells plus equal difference saying look difference vector right equal vector right look add look vector add vector minus vector let interesting let minus equal minus equal color right minus minus minus minus minus minus minus going let start going minus looks like exact vector remember doesn matter start pointing opposite direction shifted draw right exact minus opposite direction general good thing know kind negatives actually let point clear know drew actually let draw draw right non standard position negative negative minus minus start minus minus minus look like minus looks like parallel magnitude pointing exact opposite direction good thing kind brain intuition things kind finish kind idea adding subtracting vectors far want generalize generalize vector spaces aren normally intuitive actually visualize let define couple vectors let define vector equal minus let define vector equal minus addition subtraction operations hard visualize vector form useful think dimensions times vector minus times going equal vector going equal rewrite times column vector minus minus times minus times minus going equal term right times going pen tablet work going right times going times minus times times minus yellow minus times times minus minus times times isn good board let doesn write right haven figured problem right minus minus minus minus minus negative minus plus minus minus read says bizarre times minus vector multiply subtract times vector vector represent kind easy kind graph able format useful concept going later apply vectors multi dimensional spaces'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtitles[753]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In the last video I was a little\\nformal in defining what Rn is, and what a vector is,\\nand what vector addition or scalar multiplication is. In this video I want to kind of\\ngo back to basics and just give you a lot of examples. And give you a more tangible\\nsense for what vectors are and how we operate with them. So let me define a couple\\nof vectors here. And I'm going to do, most of my\\nvectors I'm going to do in this video are going\\nto be in R2. And that's because they're\\neasy to draw. Remember R2 is the set\\nof all 2-tuples. Ordered 2-tuples where each of\\nthe numbers, so you know you could have x1, my 1 looks like a\\ncomma, x1 and x2, where each of these are real numbers. So you each of them, x1 is a\\nmember of the reals, and x2 is a member of the reals. And just to give you a sense\\nof what that means, if this right here is my coordinate\\naxes, and I wanted a plot all my x1's, x2's. You know you could view this\\nas the first coordinate. We always imagine that\\nas our x-axis. And then our second coordinate\\nwe plotted on the vertical axis. That traditionally is our\\ny-axis, but we'll just call that the second number\\naxis, whatever. You could visually represent all\\nof R2 by literally every single point on this plane if\\nwe were to continue off to infinity in every direction. That's what R2 is. R1 would just be points\\njust along one of these number lines. That would be R1. So you could immediately\\nsee that R2 is kind of a bigger space. But anyway, I said that I\\nwouldn't be too abstract, that I would show you examples. So let's get some vectors\\ngoing in R2. So let me define my vector a. I'll make it nice and bold. My vector a is equal to,\\nI'll make some numbers up, negative 1, 2. And my vector b, make it nice\\nand bold, let me make that, I don't know, 3, 1. Those are my two vectors. Let's just add them up\\nand see what we get. Just based on my definition\\nof vector addition. I'll just stay in one color\\nfor now so I don't have to keep switching back and forth. So a, nice deep a, plus bolded\\nb is equal to, I just add up each of those terms.\\nNegative 1 plus 3. And then 2 plus 1. That was my definition\\nof vector addition. So that is going to be\\nequal to 2 and 3. Fair enough that just\\ncame out of my definition of vector addition. But how can we represent\\nthis vector? So we already know that if we\\nhave coordinates, you know, if I have the coordinate, and this\\nis just a convention. It's just the way\\nthat we do it. The way we visualize things. If I wanted to plot the\\npoint 1, 1, I go to my coordinate axes. The first point I go along\\nthe horizontal, what we traditionally call our x-axis. And I go 1 in that direction. And then convention is, the\\nsecond point I go 1 in the vertical direction. So the point 1, 1. Oh, sorry, let me\\nbe very clear. This is 2 and 2, so one\\nis right here, and one is right there. So the point 1, 1 would\\nbe right there. That's just the standard\\nconvention. Now our convention for\\nrepresenting vectors are, you might be tempted to say, oh,\\nmaybe I just represent this vector at the point\\nminus 1, 2. And on some level\\nyou can do that. I'll show you in a second. But the convention for vectors\\nis that you can start at any point. Let's say we're dealing with\\ntwo dimensional vectors. You can start at any\\npoint in R2. So let's say that you're\\nstarting at the point x1, and x2. This could be any point in R2. To represent the vector, what\\nwe do is we draw a line from that point to the point x1. And let me call this, let's say\\nthat we wanted to draw a. So x1 minus 1. So this is, I'm representing\\na. So this is, I want to represent\\nthe vector a. x1 minus 1, and then\\nx1 plus 2. Now if that seems confusing to\\nyou, when I draw it, it'll be very obvious. So let's say I just want to\\nstart at the point, let's just say for quirky reasons, I just\\npick a random point here. I just pick a point. That one right there. That's my starting point. So minus 4, 4. Now if I want to represent my\\nvector a, what I just said is that I add the first term\\nin vector a to my first coordinate. So x1 plus minus 1\\nor x1 minus 1. So my new one is going to be,\\nso this is my x1 minus 4. So now it's going to be, let's\\nsee, I'm starting at the point minus 4 comma 4. If I want to represent a, what\\nI do is, I draw an arrow to minus 4 plus this first\\nterm, minus 1. And then 4 plus the\\nsecond term. 4 plus 2. And so this is what? This is minus 5 comma 6. So I go to minus 5 comma 6. So I go to that point right\\nthere and I just draw a line. So my vector will\\nlook like this. I draw a line from\\nthere to there. And I draw an arrow\\nat the end point. So that's one representation\\nof the vector minus 1, 2. Actually let me do it\\na little bit better. Because minus 5 is actually\\nmore, a little closer to right here. Minus 5 comma 6 Is right\\nthere, so I draw my vector like that. But remember this point minus\\n4 comma 4 was an arbitrary place to draw my vector. I could have started\\nat this point here. I could have started at the\\npoint 4 comma 6 and done the same thing. I could have gone minus 1 in\\nthe horizontal direction, that's my movement in the\\nhorizontal direction. And then plus 2 in the\\nvertical direction. So I could have drawn, so minus\\n1 in the horizontal and plus 2 in the vertical\\ngets me right there. So I could have just as easily\\ndrawn my vector like that. These are both interpretations\\nof the same vector a. I should draw them in the\\ncolor of vector a. So vector a was this light\\nblue color right there. So this is vector a. This is vector a. Sometimes there'll\\nbe a little arrow notation over the vector. But either of those vectors. I could draw an infinite\\nnumber of vector a's. I could draw vector a here. I could draw it like that. Vector a, it goes\\nback 1 and up 2. So vector a could\\nbe right there. Similarly vector b. What does vector b do? I could pick some arbitrary\\npoint for vector b. It goes to the right 3, so it\\ngoes to the right 1, 2, 3 and then it goes up 1. So vector b, one representation\\nof vector b, looks like this. Another represention. I can start it right here. I could go to the right 3,\\n1, 2, 3, and then up 1. This would be another\\nrepresentation of my vector b. There's an infinite number of\\nrepresentations of them. But the convention is to often\\nput them in what's called the standard position. And that's to start\\nthem off at 0, 0. So your initial point, let\\nme write this down. Standard position is just to\\nstart the vectors at 0, 0 and then draw them. So vector a in standard\\nposition, I'd start at 0, 0 like that and I would go\\nback 1 and then up 2. So this is vector a in standard\\nposition right there. And then vector b in\\nstandard position. Let me write that. That's a. And then vector b in standard\\nposition is 3, go to the 3 right and then up 1. These are the vectors in\\nstandard position, but any of these other things we drew\\nare just as valid. Now let's see if we can get\\nan interpretation of what happened when we\\nadded a plus b. Well if I draw that vector in\\nstandard position, I just calculated, it's 2, 3. So I go to the right\\n2 and I go up 3. So if I just draw it in\\nstandard position it looks like this. This vector right there. And at first when you look at\\nit, this vector right here is the vector a plus b in\\nstandard position. When you draw it like that,\\nit's not clear what the relationship is when\\nwe added a and b. But to see the relationship what\\nyou do is, you put a and b head to tails. What that means is, you put\\nthe tail end of b to the front end of a. Because remember, all\\nof these are valid representations of b. All of the representations\\nof the vector b. They all have, they're all\\nparallel to each other, but they can start from anywhere. So another equally valid\\nrepresentation of vector b is to start at this point right\\nhere, kind of the end point of vector a in standard position,\\nand then draw vector b starting from there. So you go 3 to the right. So you go 1, 2, 3. And then you go up 1. So vector b could also be\\ndrawn just like that. And then you should\\nsee something interesting had happened. And remember, this vector b\\nrepresentation is not in standard position, but it's just\\nan equally valid way to represent my vector. Now what do you see? When I add a, which is right\\nhere, to b what do I get if I connect the starting point of\\na with the end point of b? I get the addition. I have added the two vectors. And I could have done\\nthat anywhere. I could have started\\nwith a here. And then I could have\\ndone the end point. I could have started b here and\\ngone 3 to the right, 1, 2, 3 and then up 1. And I could have drawn b\\nright there like that. And then if I were to add a plus\\nb, I go to the starting point of a, and then\\nthe end point of b. And that should also\\nbe the visual representation of a plus b. Just to make sure it confirms\\nwith this number, what I did here was I went 2 to\\nthe right, 1, 2 and then I went 3 up. 1, 2, 3 and I got a plus b. Now let's think about\\nwhat happens when we scale our vectors. When we multiply it times\\nsome scalar factor. So let me pick new vectors. Those have gotten monotonous. Let me define vector v. v for vector. Let's say that it is\\nequal to 1, 2. So if I just wanted to draw\\nvector v in standard position, I would just go 1 to\\nthe horizontal and then 2 to the vertical. That's it. That's the vector in\\nstandard position. If I wanted to do it in a non\\nstandard position, I could do it right here. 1 to the right up 2,\\njust like that. Equally valid way of\\ndrawing vector v. Equally valid way of doing it. Now what happens if I\\nmultiply vector v. What if I have, I don't know,\\nwhat if I have 2 times v? 2 times my vector v is now going\\nto be equal to 2 times each of these terms. So it's\\ngoing to be 2 times 1 which is 2, and then 2 times\\n2 which is 4. Now what does 2 times\\nvector v look like? Well let me just start from\\nan arbitrary position. Let me just start\\nright over here. So I'm going to go 2\\nto the right, 1, 2. And I go up 4. 1, 2, 3, 4. So this is what 2 times\\nvector v looks like. This is 2 times my vector v. And if you look at it, it's\\npointing in the exact same direction but now it's\\ntwice as long. And that makes sense because we\\nscaled it by a factor of 2. When you multiply it by a\\nscalar, or you're not changing its direction. Its direction is the exact same\\nthing as it was before. You're just scaling\\nit by that amount. And I could draw\\nthis anywhere. I could have drawn\\nit right here. I could have drawn 2v\\nright on top of v. Then you would have seen it,\\nI don't want to cover it. You would have seen that it\\ngoes, it's exactly, in this case when I draw it in standard position, it's colinear. It's along the same line,\\nit's just twice as far. it's just twice as long\\nbut they have the exact same direction. Now what happens if I were\\nto multiply minus 4 times our vector v? Well then that will be equal\\nto minus 4 times 1, which is minus 4. And then minus 4 times\\n2, which is minus 8. So this is on my new vector. Minus 4, minus 8. This is minus 4 times\\nour vector v. So let's just start at\\nsome arbitrary point. Let's just do it in\\nstandard position. So you go to the right 4. Or you go to the left 4. So so you go to the left\\n4, 1, 2, 3, 4. And then down 8. Looks like that. So this new vector is going\\nto look like this. Let me try and draw a relatively\\nstraight line. There you go. So this is minus 4 times\\nour vector v. I'll draw a little arrow\\non it to make sure you know it's a vector. Now what happened? Well we're kind of in\\nthe same direction. Actually we're in the exact\\nopposite direction. But we're still along the\\nsame line, right? But we're just in the exact\\nopposite direction. And it's this negative right\\nthere that flipped us around. If we just multiplied negative\\n1 times this, we would have just flipped around to\\nright there, right? But we multiplied it\\nby negative 4. So we scaled it by 4, so you\\nmake it 4 times as long, and then it's negative, so\\nthen it flips around. It flips backwards. So now that we have that notion,\\nwe can kind of start understanding the idea of\\nsubtracting vectors. Let me make up 2 new\\nvectors right now. Let's say my vector x, nice and\\nbold x, is equal to, and I'm doing everything in R2, but\\nin the last part of this video I'll make a few examples\\nin R3 or R4. Let's say my vector x\\nis equal to 2, 4. And let's say I have\\na vector y. y, make it nice and bold. And then that is equal to\\nnegative 1, minus 2. And I want to think about\\nthe notion of what x minus y is equal to. Well we can say that this is the\\nsame thing as x plus minus 1 times our vector y. Right? So x plus minus 1 times\\nour vector y. Now we can use our\\ndefinitions. We know how to multiply\\nby a scalar. So we'll say that this\\nis equal to, let me switch colors. I don't like this color. This is equal to our\\nx vector is 2, 4. And then what's minus\\n1 times y? So minus 1 times y is minus\\n1 times minus 1 is 1. And then minus 1 times\\nminus 2 is 2. So x minus y is going to be\\nthese two vectors added to each other, right? I'm just adding the\\nminus of y. This is minus vector y. So this x minus y is going to\\nbe equal to 3 and 3 and 6. So let's see what that looks\\nlike when we visually represent them. Our vector x was 2, 4. So 2, 4 in standard position\\nit looks like this. That's my vector x. And then vector y in standard\\nposition, let me do it in a different color, I'll\\ndo y in green. Vector y is minus 1, minus 2. It looks just like this. And actually I ended up\\ninadvertently doing collinear vectors, but, hey, this\\nis interesting too. So this is vector y. So then what's their\\ndifference? This is 3, 6. So it's the vector 3, 6. So it's this vector. Let me draw it someplace else. If I start here I go 1, 2, 3. And then I go up 6. So then up 6. It's a vector that\\nlooks like this. That's the difference between\\nthe two vectors. So at first you say,\\nthis is x minus y. Hey, how is this the difference\\nof these two? Well if you overlay this. If you just shift this over\\nthis, you could actually just start here and go straight up. And you'll see that it's really\\nthe difference between the end points. You're kind of connecting\\nthe end points. I actually didn't want to\\ndraw collinear vectors. Let me do another example. Although that one's kind\\nof interesting. You often don't see that\\none in a book. Let me to define vector x\\nin this case to be 2, 3. And let me define vector y\\nto be minus 4, minus 2. So what would be x in\\nstandard position? It would be 2, 3. It'd look like that. That is our vector x if we\\nstart at the origin. So this is x. And then what does vector\\ny look like? I'll do y in orange. Minus 4, minus 2. So vector y looks like this. Now what is x minus y? Well you know, we could\\nview this, 2 plus minus 1 times this. We could just say\\n2 minus minus 4. I think you get the idea now. But we just did it the first\\nway the last time because I wanted to go from my basic\\ndefinitions of scalar multiplication. So x minus y is just going to\\nbe equal to 2 plus minus 1 times minus 4, or\\n2 minus minus 4. That's the same thing as\\n2 plus 4, so it's 6. And then it's 3 minus\\nminus 2, so it's 5. Right? So the difference between the\\ntwo is the vector 6, 5. So you could draw it\\nout here again. So you could go, add 6 to 4, go\\nup there, then to 5, you'd go like that. So the vector would look\\nsomething like this. It shouldn't curve like that,\\nso that's x minus y. But if we drew them between,\\nlike in the last example, I showed that you could draw it\\nbetween their two heads. So if you do it here, what\\ndoes it look like? Well if you start at this point\\nright there and you go 6 to the right and then up 5,\\nyou end up right there. So the difference between the\\ntwo vectors, let me make sure I get it, the difference\\nbetween the two vectors looks like that. It looks just like that. Which kind of should make\\nsense intuitively. x minus y. That's the difference between\\nthe two vectors. You can view the difference as,\\nhow do you get from one vector to another\\nvector, right? Like if, you know, let's go\\nback to our kind of second grade world of just scalars. If I say what 7 minus 5 is, and\\nyou say it's equal to 2, well that just tells you that\\n5 plus 2 is equal to 7. Or the difference between\\n5 and 7 is 2. And here you're saying, look the\\ndifference between x and y is this vector right there. It's equal to that vector\\nright there. Or you could say look, if I\\ntake 5 and add 2 I get 7. Or you could say, look, if I\\ntake vector y, and I add vector x minus y, then\\nI get vector x. Now let's do something else\\nthat's interesting. Let's do what y minus\\nx is equal to. y minus x. What is that equal to? Do it in another color\\nright here. Well we'll take minus 4, minus\\n2 which is minus 6. And then you have minus\\n2, minus 3. It's minus 5. So y minus x is going to be,\\nlet's see, if we start here we're going to go down 6. 1, 2, 3, 4, 5, 6. And then back 5. So back 2, 4, 5. So y minus x looks like this. It's really the exact\\nsame vector. Remember, it doesn't matter\\nwhere we start. It's just pointing in the\\nopposite direction. So if we shifted it here. I could draw it right\\non top of this. It would be the exact as x\\nminus y, but just in the opposite direction. Which is just a general\\ngood thing to know. So you can kind of do them as\\nthe negatives of each other. And actually let me make\\nthat point very clear. You know we drew y. Actually let me draw x, x\\nwe could draw as 2, 3. So you go to the right\\n2 and then up 3. I've done this before. This is x in non standard\\nposition. That's x as well. What is negative x? Negative x is minus 2 minus 3. So if I were to start here,\\nI'd go to minus 2, then I'd go minus 3. So minus x would look\\njust like this. Minus x. It looks just like x. It's parallel. It has the same magnitude. It's just pointing in the exact\\nopposite direction. And this is just a good thing\\nto kind of really get seared into your brain is to have an\\nintuition for these things. Now just to kind of finish up\\nthis kind of idea of adding and subtracting vectors. Everything I did so\\nfar was in R2. But I want to show you that\\nwe can generalize them. And we can even generalize them\\nto vector spaces that aren't normally intuitive for\\nus to actually visualize. So let me define a couple\\nof vectors. Let me define vector a to be\\nequal to 0, minus 1, 2, and 3. Let me define vector b to be\\nequal to 4, minus 2, 0, 5. We can do the same addition\\nand subtraction operations with them. It's just it'll be hard\\nto visualize. We can keep them in\\njust vector form. So that it's still useful to\\nthink in four dimensions. So if I were to say 4 times a. This is the vector a\\nminus 2 times b. What is this going\\nto be equal to? This is a vector. What is this going\\nto be equal to? Well we could rewrite this as\\n4 times this whole column vector, 0, minus 1, 2, and 3. Minus 2 times b. Minus 2 times 4,\\nminus 2, 0, 5. And what is this going\\nto be equal to? This term right here, 4 times\\nthis, you're going to get, the pen tablet seems to not work\\nwell there, so I'm going to do it right here. 4 times this, you're going to\\nget 4 times 0, 0, minus 4, 8. 4 times 2 is 8. 4 times 3 is 12. And then minus, I'll do it in\\nyellow, minus 2 times 4 is 8. 2 times minus 2 is minus 4. 2 times 0 is 0. 2 times 5 is 10. This isn't a good part of my\\nboard, so let me just. It doesn't write well\\nright over there. I haven't figured out the\\nproblem, but if I were just right it over here,\\nwhat do we get? With 0 minus 8? Minus 8. Minus 4, minus 4. Minus negative 4. So that's minus 4 plus\\n4, so that's 0. 8 minus 0 is 8. 12 minus, what was this? I can't even read it,\\nwhat it says. Oh, this is a 10. Now you can see it again. Something is very bizarre. 2 times 5 is 10. So it's 12 minus\\n10, so it's 2. So when we take this vector\\nand multiply it by 4, and subtract 2 times this vector,\\nwe just get this vector. And even though you can't\\nrepresent this in kind of an easy kind of graph-able format, this is a useful concept. And we're going to see this\\nlater when we apply some of these vectors to\\nmulti-dimensional spaces.\""
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[753,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>The following\\ncontent is provided under a Cre...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>OPERATOR: The following content\\nis provided u...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>NARRATOR: The following content\\nis provided u...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Let us now revisit the second\\ncalculation tha...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>The following content is provided under a\\nCre...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>The following\\ncontent is provided under a Cre...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>The following\\ncontent is provided under a Cre...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>The following\\ncontent is provided under a Cre...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>The following\\ncontent is provided under a Cre...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>ANNOUNCER: Open content is\\nprovided under a c...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>&amp;gt;&amp;gt; [MUSIC PLAYING] &amp;gt;&amp;gt; DAVID J. MAL...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>The following\\ncontent is provided under a Cre...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>The following\\ncontent is provided under a cre...</td>\n",
       "      <td>Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>OPERATOR: -- The following content is\\nprovide...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>The following\\ncontent is provided under a Cre...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>The following\\ncontent is provided under a Cre...</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text             label\n",
       "13   The following content is\\nprovided under a Cre...        Statistics\n",
       "16   The following content is\\nprovided under a Cre...  Computer Science\n",
       "36   The following content is\\nprovided under a Cre...              Math\n",
       "49   The following content is\\nprovided under a Cre...              Math\n",
       "52   The following\\ncontent is provided under a Cre...        Statistics\n",
       "56   OPERATOR: The following content\\nis provided u...              Math\n",
       "67   NARRATOR: The following content\\nis provided u...        Statistics\n",
       "104  The following content is\\nprovided under a Cre...        Statistics\n",
       "126  The following content is\\nprovided under a Cre...        Statistics\n",
       "141  Let us now revisit the second\\ncalculation tha...        Statistics\n",
       "165  The following content is\\nprovided under a Cre...  Computer Science\n",
       "194  The following content is\\nprovided under a Cre...  Computer Science\n",
       "198  The following content is\\nprovided under a Cre...        Statistics\n",
       "232  The following content is provided under a\\nCre...  Computer Science\n",
       "244  The following content is\\nprovided under a Cre...  Computer Science\n",
       "264  The following\\ncontent is provided under a Cre...              Math\n",
       "289  The following\\ncontent is provided under a Cre...  Computer Science\n",
       "304  The following\\ncontent is provided under a Cre...        Statistics\n",
       "306  The following content is\\nprovided under a Cre...  Computer Science\n",
       "350  The following content is\\nprovided under a Cre...              Math\n",
       "361  The following\\ncontent is provided under a Cre...              Math\n",
       "370  The following content is\\nprovided under a Cre...              Math\n",
       "447  The following content is\\nprovided under a Cre...              Math\n",
       "458  ANNOUNCER: Open content is\\nprovided under a c...              Math\n",
       "467  The following content is\\nprovided under a Cre...  Computer Science\n",
       "513  &gt;&gt; [MUSIC PLAYING] &gt;&gt; DAVID J. MAL...  Computer Science\n",
       "566  The following content is\\nprovided under a Cre...  Computer Science\n",
       "576  The following\\ncontent is provided under a Cre...              Math\n",
       "632  The following content is\\nprovided under a Cre...        Statistics\n",
       "639  The following content is\\nprovided under a Cre...              Math\n",
       "663  The following\\ncontent is provided under a cre...        Statistics\n",
       "685  OPERATOR: -- The following content is\\nprovide...  Computer Science\n",
       "712  The following content is\\nprovided under a Cre...              Math\n",
       "771  The following\\ncontent is provided under a Cre...              Math\n",
       "783  The following\\ncontent is provided under a Cre...              Math\n",
       "786  The following content is\\nprovided under a Cre...  Computer Science"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['text'].str.contains('divide and conquer')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>count(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Computer Science</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Math</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Statistics</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label  count(*)\n",
       "0  Computer Science        13\n",
       "1              Math        14\n",
       "2        Statistics        10"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "q = \"\"\"SELECT label, count(*)\n",
    "       FROM df\n",
    "       where text like '%divide and conquer%' \n",
    "       group by label\n",
    "       \"\"\"\n",
    "\n",
    "pysqldf(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>count(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Computer Science</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Math</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Statistics</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label  count(*)\n",
       "0  Computer Science        60\n",
       "1              Math        75\n",
       "2        Statistics        45"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "q = \"\"\"SELECT label, count(*)\n",
    "       FROM df\n",
    "       where text like '%distribution%' \n",
    "       group by label\n",
    "       \"\"\"\n",
    "\n",
    "pysqldf(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
